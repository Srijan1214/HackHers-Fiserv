{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from pathlib import Path\n",
    "df=pd.read_excel(\"D:/College\\Machine Learning/heckhers credit card default prediction/default of credit card clients.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "(30000, 24)\n"
    },
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X15</th>\n      <th>X16</th>\n      <th>X17</th>\n      <th>X18</th>\n      <th>X19</th>\n      <th>X20</th>\n      <th>X21</th>\n      <th>X22</th>\n      <th>X23</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>20000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>24</td>\n      <td>2</td>\n      <td>2</td>\n      <td>-1</td>\n      <td>-1</td>\n      <td>-2</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>689</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>120000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>26</td>\n      <td>-1</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>3272</td>\n      <td>3455</td>\n      <td>3261</td>\n      <td>0</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>0</td>\n      <td>2000</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>90000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>2</td>\n      <td>34</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>14331</td>\n      <td>14948</td>\n      <td>15549</td>\n      <td>1518</td>\n      <td>1500</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>1000</td>\n      <td>5000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>50000</td>\n      <td>2</td>\n      <td>2</td>\n      <td>1</td>\n      <td>37</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>28314</td>\n      <td>28959</td>\n      <td>29547</td>\n      <td>2000</td>\n      <td>2019</td>\n      <td>1200</td>\n      <td>1100</td>\n      <td>1069</td>\n      <td>1000</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>50000</td>\n      <td>1</td>\n      <td>2</td>\n      <td>1</td>\n      <td>57</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>-1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>20940</td>\n      <td>19146</td>\n      <td>19131</td>\n      <td>2000</td>\n      <td>36681</td>\n      <td>10000</td>\n      <td>9000</td>\n      <td>689</td>\n      <td>679</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 24 columns</p>\n</div>",
      "text/plain": "       X1 X2 X3 X4  X5  X6 X7  X8  X9 X10  ...    X15    X16    X17   X18  \\\n1   20000  2  2  1  24   2  2  -1  -1  -2  ...      0      0      0     0   \n2  120000  2  2  2  26  -1  2   0   0   0  ...   3272   3455   3261     0   \n3   90000  2  2  2  34   0  0   0   0   0  ...  14331  14948  15549  1518   \n4   50000  2  2  1  37   0  0   0   0   0  ...  28314  28959  29547  2000   \n5   50000  1  2  1  57  -1  0  -1   0   0  ...  20940  19146  19131  2000   \n\n     X19    X20   X21   X22   X23  Y  \n1    689      0     0     0     0  1  \n2   1000   1000  1000     0  2000  1  \n3   1500   1000  1000  1000  5000  0  \n4   2019   1200  1100  1069  1000  0  \n5  36681  10000  9000   689   679  0  \n\n[5 rows x 24 columns]"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df.drop(index=0)\n",
    "df=df.drop(df.columns[0],axis=1)\n",
    "print(df.shape)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X1 has 81 unique values.\nX2 has 2 unique values.\nX3 has 7 unique values.\nX4 has 4 unique values.\nX5 has 56 unique values.\nX6 has 11 unique values.\nX7 has 11 unique values.\nX8 has 11 unique values.\nX9 has 11 unique values.\nX10 has 10 unique values.\nX11 has 10 unique values.\nX12 has 22723 unique values.\nX13 has 22346 unique values.\nX14 has 22026 unique values.\nX15 has 21548 unique values.\nX16 has 21010 unique values.\nX17 has 20604 unique values.\nX18 has 7943 unique values.\nX19 has 7899 unique values.\nX20 has 7518 unique values.\nX21 has 6937 unique values.\nX22 has 6897 unique values.\nX23 has 6939 unique values.\nY has 2 unique values.\n"
    }
   ],
   "source": [
    "#Finding columns that have string values.\n",
    "\n",
    "for col_name in df.columns:\n",
    "    if df[col_name].dtypes=='object':\n",
    "        a=df[col_name].unique()\n",
    "        a=len(a)\n",
    "        print(col_name + \" has \"+str(a)+\" unique values.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>X1</th>\n      <th>X2</th>\n      <th>X3</th>\n      <th>X4</th>\n      <th>X5</th>\n      <th>X6</th>\n      <th>X7</th>\n      <th>X8</th>\n      <th>X9</th>\n      <th>X10</th>\n      <th>...</th>\n      <th>X15</th>\n      <th>X16</th>\n      <th>X17</th>\n      <th>X18</th>\n      <th>X19</th>\n      <th>X20</th>\n      <th>X21</th>\n      <th>X22</th>\n      <th>X23</th>\n      <th>Y</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.010101</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.051724</td>\n      <td>0.4</td>\n      <td>0.4</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.160138</td>\n      <td>0.080648</td>\n      <td>0.260979</td>\n      <td>0.000000</td>\n      <td>0.000409</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.111111</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.086207</td>\n      <td>0.1</td>\n      <td>0.4</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.163220</td>\n      <td>0.084074</td>\n      <td>0.263485</td>\n      <td>0.000000</td>\n      <td>0.000594</td>\n      <td>0.001116</td>\n      <td>0.001610</td>\n      <td>0.000000</td>\n      <td>0.003783</td>\n      <td>1.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.080808</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.224138</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.173637</td>\n      <td>0.095470</td>\n      <td>0.272928</td>\n      <td>0.001738</td>\n      <td>0.000891</td>\n      <td>0.001116</td>\n      <td>0.001610</td>\n      <td>0.002345</td>\n      <td>0.009458</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.040404</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.275862</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.186809</td>\n      <td>0.109363</td>\n      <td>0.283685</td>\n      <td>0.002290</td>\n      <td>0.001199</td>\n      <td>0.001339</td>\n      <td>0.001771</td>\n      <td>0.002506</td>\n      <td>0.001892</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.040404</td>\n      <td>0.0</td>\n      <td>0.333333</td>\n      <td>0.333333</td>\n      <td>0.620690</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.179863</td>\n      <td>0.099633</td>\n      <td>0.275681</td>\n      <td>0.002290</td>\n      <td>0.021779</td>\n      <td>0.011160</td>\n      <td>0.014493</td>\n      <td>0.001615</td>\n      <td>0.001284</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.040404</td>\n      <td>0.0</td>\n      <td>0.166667</td>\n      <td>0.666667</td>\n      <td>0.275862</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.178407</td>\n      <td>0.100102</td>\n      <td>0.276367</td>\n      <td>0.002862</td>\n      <td>0.001078</td>\n      <td>0.000733</td>\n      <td>0.001610</td>\n      <td>0.002345</td>\n      <td>0.001513</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.494949</td>\n      <td>0.0</td>\n      <td>0.166667</td>\n      <td>0.666667</td>\n      <td>0.137931</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.671310</td>\n      <td>0.559578</td>\n      <td>0.625196</td>\n      <td>0.062961</td>\n      <td>0.023749</td>\n      <td>0.042409</td>\n      <td>0.032591</td>\n      <td>0.032237</td>\n      <td>0.026047</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.090909</td>\n      <td>1.0</td>\n      <td>0.333333</td>\n      <td>0.666667</td>\n      <td>0.034483</td>\n      <td>0.2</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.160346</td>\n      <td>0.080490</td>\n      <td>0.261414</td>\n      <td>0.000435</td>\n      <td>0.000357</td>\n      <td>0.000000</td>\n      <td>0.000936</td>\n      <td>0.003955</td>\n      <td>0.002917</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.131313</td>\n      <td>1.0</td>\n      <td>0.500000</td>\n      <td>0.333333</td>\n      <td>0.120690</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.4</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.171640</td>\n      <td>0.092342</td>\n      <td>0.263837</td>\n      <td>0.003811</td>\n      <td>0.000000</td>\n      <td>0.000482</td>\n      <td>0.001610</td>\n      <td>0.002345</td>\n      <td>0.001892</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.010101</td>\n      <td>0.0</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>0.241379</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.1</td>\n      <td>...</td>\n      <td>0.160138</td>\n      <td>0.093545</td>\n      <td>0.271670</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.020945</td>\n      <td>0.002631</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>0.191919</td>\n      <td>1.0</td>\n      <td>0.500000</td>\n      <td>0.666667</td>\n      <td>0.224138</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>0.4</td>\n      <td>0.2</td>\n      <td>0.2</td>\n      <td>...</td>\n      <td>0.162505</td>\n      <td>0.082461</td>\n      <td>0.263846</td>\n      <td>0.002640</td>\n      <td>0.000007</td>\n      <td>0.000056</td>\n      <td>0.000483</td>\n      <td>0.008764</td>\n      <td>0.000125</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.252525</td>\n      <td>1.0</td>\n      <td>0.166667</td>\n      <td>0.666667</td>\n      <td>0.517241</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>0.1</td>\n      <td>...</td>\n      <td>0.168161</td>\n      <td>0.102747</td>\n      <td>0.271482</td>\n      <td>0.024976</td>\n      <td>0.005917</td>\n      <td>0.009579</td>\n      <td>0.035911</td>\n      <td>0.000000</td>\n      <td>0.006885</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>12 rows Ã— 24 columns</p>\n</div>",
      "text/plain": "          X1   X2        X3        X4        X5   X6   X7   X8   X9  X10  ...  \\\n0   0.010101  1.0  0.333333  0.333333  0.051724  0.4  0.4  0.1  0.1  0.0  ...   \n1   0.111111  1.0  0.333333  0.666667  0.086207  0.1  0.4  0.2  0.2  0.2  ...   \n2   0.080808  1.0  0.333333  0.666667  0.224138  0.2  0.2  0.2  0.2  0.2  ...   \n3   0.040404  1.0  0.333333  0.333333  0.275862  0.2  0.2  0.2  0.2  0.2  ...   \n4   0.040404  0.0  0.333333  0.333333  0.620690  0.1  0.2  0.1  0.2  0.2  ...   \n5   0.040404  0.0  0.166667  0.666667  0.275862  0.2  0.2  0.2  0.2  0.2  ...   \n6   0.494949  0.0  0.166667  0.666667  0.137931  0.2  0.2  0.2  0.2  0.2  ...   \n7   0.090909  1.0  0.333333  0.666667  0.034483  0.2  0.1  0.1  0.2  0.2  ...   \n8   0.131313  1.0  0.500000  0.333333  0.120690  0.2  0.2  0.4  0.2  0.2  ...   \n9   0.010101  0.0  0.500000  0.666667  0.241379  0.0  0.0  0.0  0.0  0.1  ...   \n10  0.191919  1.0  0.500000  0.666667  0.224138  0.2  0.2  0.4  0.2  0.2  ...   \n11  0.252525  1.0  0.166667  0.666667  0.517241  0.1  0.1  0.1  0.1  0.1  ...   \n\n         X15       X16       X17       X18       X19       X20       X21  \\\n0   0.160138  0.080648  0.260979  0.000000  0.000409  0.000000  0.000000   \n1   0.163220  0.084074  0.263485  0.000000  0.000594  0.001116  0.001610   \n2   0.173637  0.095470  0.272928  0.001738  0.000891  0.001116  0.001610   \n3   0.186809  0.109363  0.283685  0.002290  0.001199  0.001339  0.001771   \n4   0.179863  0.099633  0.275681  0.002290  0.021779  0.011160  0.014493   \n5   0.178407  0.100102  0.276367  0.002862  0.001078  0.000733  0.001610   \n6   0.671310  0.559578  0.625196  0.062961  0.023749  0.042409  0.032591   \n7   0.160346  0.080490  0.261414  0.000435  0.000357  0.000000  0.000936   \n8   0.171640  0.092342  0.263837  0.003811  0.000000  0.000482  0.001610   \n9   0.160138  0.093545  0.271670  0.000000  0.000000  0.000000  0.020945   \n10  0.162505  0.082461  0.263846  0.002640  0.000007  0.000056  0.000483   \n11  0.168161  0.102747  0.271482  0.024976  0.005917  0.009579  0.035911   \n\n         X22       X23    Y  \n0   0.000000  0.000000  1.0  \n1   0.000000  0.003783  1.0  \n2   0.002345  0.009458  0.0  \n3   0.002506  0.001892  0.0  \n4   0.001615  0.001284  0.0  \n5   0.002345  0.001513  0.0  \n6   0.032237  0.026047  0.0  \n7   0.003955  0.002917  0.0  \n8   0.002345  0.001892  0.0  \n9   0.002631  0.000000  0.0  \n10  0.008764  0.000125  0.0  \n11  0.000000  0.006885  0.0  \n\n[12 rows x 24 columns]"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df=df_scaled=pd.DataFrame(scaler.fit_transform(df),columns=df.columns)\n",
    "df_scaled.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "24\n"
    }
   ],
   "source": [
    "column_names=list(df) \n",
    "print(len(column_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.sort_values(by=['Y'])\n",
    "\n",
    "df = df.tail(13200)\n",
    "df=df.sample(frac=1)\n",
    "\n",
    "X = df.drop('Y',1)\n",
    "y = df['Y']\n",
    "df=df.drop(df.columns[-1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "X1   X2        X3        X4        X5   X6   X7   X8   X9  X10  \\\n17808  0.000000  0.0  0.333333  0.666667  0.051724  0.5  0.4  0.4  0.5  0.5   \n9043   0.050505  0.0  0.166667  0.666667  0.172414  0.3  0.0  0.0  0.1  0.2   \n27447  0.131313  0.0  0.500000  0.666667  0.206897  0.0  0.0  0.0  0.0  0.0   \n27111  0.060606  0.0  0.500000  0.333333  0.431034  0.3  0.4  0.4  0.4  0.4   \n11739  0.292929  1.0  0.166667  0.333333  0.448276  0.4  0.4  0.4  0.4  0.4   \n10483  0.101010  0.0  0.500000  0.333333  0.379310  0.1  0.1  0.1  0.2  0.2   \n3459   0.222222  0.0  0.333333  0.666667  0.155172  0.2  0.2  0.4  0.2  0.2   \n11242  0.141414  0.0  0.166667  0.666667  0.086207  0.2  0.2  0.4  0.4  0.4   \n8740   0.202020  0.0  0.166667  0.666667  0.224138  0.2  0.2  0.2  0.2  0.2   \n28426  0.040404  1.0  0.166667  0.666667  0.155172  0.3  0.4  0.4  0.4  0.4   \n27338  0.434343  0.0  0.166667  0.666667  0.206897  0.1  0.1  0.1  0.1  0.2   \n17132  0.070707  1.0  0.166667  0.666667  0.051724  0.2  0.2  0.4  0.2  0.2   \n6751   0.020202  0.0  0.166667  0.333333  0.189655  0.2  0.2  0.2  0.2  0.2   \n5426   0.040404  1.0  0.333333  0.333333  0.482759  0.1  0.1  0.1  0.1  0.1   \n3061   0.060606  0.0  0.166667  0.666667  0.103448  0.2  0.2  0.2  0.2  0.2   \n9246   0.151515  0.0  0.333333  0.333333  0.275862  0.4  0.1  0.1  0.1  0.1   \n2337   0.222222  1.0  0.500000  0.333333  0.413793  0.1  0.1  0.0  0.1  0.2   \n2967   0.111111  0.0  0.333333  0.333333  0.448276  0.1  0.1  0.1  0.2  0.2   \n13904  0.040404  1.0  0.166667  0.666667  0.086207  0.5  0.4  0.4  0.4  0.4   \n5117   0.494949  0.0  0.166667  0.666667  0.293103  0.0  0.0  0.0  0.0  0.0   \n29587  0.010101  0.0  0.333333  0.666667  0.206897  0.2  0.2  0.2  0.2  0.2   \n2152   0.060606  0.0  0.333333  0.666667  0.517241  0.2  0.2  0.2  0.2  0.2   \n13238  0.010101  0.0  0.166667  0.666667  0.017241  0.3  0.4  0.4  0.2  0.2   \n2846   0.343434  1.0  0.166667  0.666667  0.172414  0.2  0.2  0.2  0.2  0.2   \n15526  0.131313  0.0  0.500000  0.333333  0.448276  0.1  0.1  0.1  0.4  0.2   \n17264  0.232323  1.0  0.333333  0.333333  0.431034  0.2  0.2  0.2  0.2  0.0   \n13526  0.282828  1.0  0.333333  0.333333  0.137931  0.4  0.4  0.2  0.2  0.2   \n10837  0.050505  1.0  0.333333  0.333333  0.310345  0.2  0.2  0.2  0.2  0.2   \n9951   0.272727  1.0  0.333333  0.666667  0.051724  0.1  0.1  0.1  0.1  0.1   \n7289   0.010101  0.0  0.333333  0.666667  0.206897  0.2  0.2  0.2  0.2  0.2   \n...         ...  ...       ...       ...       ...  ...  ...  ...  ...  ...   \n23607  0.232323  1.0  0.166667  0.666667  0.275862  0.5  0.4  0.4  0.4  0.4   \n9404   0.101010  1.0  0.333333  0.666667  0.034483  0.2  0.2  0.2  0.2  0.2   \n28289  0.040404  1.0  0.333333  0.666667  0.258621  0.4  0.2  0.2  0.2  0.2   \n8763   0.494949  1.0  0.333333  0.666667  0.379310  0.1  0.1  0.0  0.1  0.1   \n7288   0.040404  1.0  0.333333  0.666667  0.310345  0.2  0.2  0.0  0.0  0.0   \n27861  0.202020  1.0  0.333333  0.333333  0.068966  0.2  0.2  0.2  0.2  0.2   \n27975  0.151515  1.0  0.500000  0.666667  0.103448  0.3  0.1  0.1  0.1  0.1   \n17598  0.020202  1.0  0.333333  0.333333  0.155172  0.2  0.2  0.2  0.2  0.4   \n2766   0.171717  1.0  0.333333  0.333333  0.258621  0.2  0.2  0.2  0.2  0.2   \n7679   0.040404  1.0  0.500000  0.666667  0.344828  0.4  0.4  0.4  0.4  0.4   \n13743  0.252525  0.0  0.166667  0.666667  0.137931  0.4  0.4  0.4  0.4  0.4   \n24216  0.171717  0.0  0.500000  0.000000  0.103448  0.0  0.0  0.0  0.0  0.0   \n9734   0.282828  1.0  0.333333  0.333333  0.275862  0.0  0.0  0.0  0.0  0.0   \n15095  0.494949  0.0  0.166667  1.000000  0.293103  0.0  0.0  0.0  0.0  0.0   \n1729   0.060606  0.0  0.333333  0.333333  0.344828  0.3  0.4  0.4  0.4  0.2   \n6150   0.292929  1.0  0.500000  0.666667  0.310345  0.3  0.0  0.1  0.1  0.1   \n11064  0.212121  0.0  0.166667  0.666667  0.068966  0.3  0.4  0.2  0.2  0.2   \n23387  0.353535  1.0  0.500000  0.666667  0.293103  0.1  0.1  0.1  0.1  0.2   \n16862  0.212121  0.0  0.166667  0.333333  0.241379  0.1  0.1  0.1  0.1  0.2   \n9387   0.141414  1.0  0.333333  0.333333  0.241379  0.2  0.2  0.2  0.2  0.4   \n1268   0.111111  0.0  0.333333  0.666667  0.275862  0.1  0.4  0.2  0.2  0.2   \n12402  0.262626  1.0  0.333333  0.333333  0.327586  0.2  0.2  0.2  0.2  0.2   \n2873   0.464646  1.0  0.333333  0.666667  0.206897  0.2  0.2  0.2  0.2  0.2   \n9212   0.070707  0.0  0.333333  0.333333  0.224138  0.3  0.4  0.4  0.2  0.2   \n8612   0.020202  0.0  0.166667  0.666667  0.224138  0.2  0.2  0.2  0.2  0.4   \n7842   0.111111  1.0  0.166667  0.666667  0.068966  0.3  0.0  0.0  0.0  0.0   \n3194   0.010101  1.0  0.166667  0.666667  0.293103  0.1  0.2  0.2  0.5  0.4   \n14705  0.171717  0.0  0.166667  0.666667  0.155172  0.3  0.1  0.1  0.0  0.0   \n24390  0.040404  0.0  0.333333  0.333333  0.206897  0.2  0.2  0.2  0.2  0.2   \n444    0.232323  0.0  0.166667  0.666667  0.396552  0.2  0.2  0.2  0.2  0.2   \n\n       ...       X14       X15       X16       X17       X18           X19  \\\n17808  ...  0.086921  0.161127  0.081689  0.261786  0.000000  0.000000e+00   \n9043   ...  0.084895  0.161796  0.081521  0.263369  0.000000  0.000000e+00   \n27447  ...  0.086330  0.163706  0.080648  0.268192  0.000541  0.000000e+00   \n27111  ...  0.093830  0.172494  0.094625  0.271528  0.000000  1.781199e-03   \n11739  ...  0.089090  0.164848  0.085606  0.264821  0.000000  0.000000e+00   \n10483  ...  0.120952  0.191438  0.111459  0.280218  0.005781  3.742417e-02   \n3459   ...  0.110860  0.205862  0.132683  0.304403  0.009158  0.000000e+00   \n11242  ...  0.167236  0.300681  0.231344  0.374815  0.018821  3.265531e-03   \n8740   ...  0.112609  0.207561  0.131291  0.301116  0.002290  1.781199e-03   \n28426  ...  0.110529  0.202361  0.126725  0.297542  0.002404  1.187466e-03   \n27338  ...  0.089609  0.176307  0.081814  0.275101  0.019359  3.529742e-03   \n17132  ...  0.110270  0.186954  0.106400  0.279181  0.010789  5.937329e-07   \n6751   ...  0.097658  0.179630  0.101604  0.278220  0.001532  7.843212e-04   \n5426   ...  0.087002  0.160324  0.081836  0.269584  0.010434  7.112920e-04   \n3061   ...  0.108646  0.187250  0.110674  0.283399  0.002404  1.187466e-03   \n9246   ...  0.086802  0.160922  0.081474  0.261619  0.000954  4.945795e-04   \n2337   ...  0.089486  0.169907  0.090562  0.261999  0.009656  3.535679e-03   \n2967   ...  0.087911  0.162866  0.081787  0.262330  0.000298  3.415745e-03   \n13904  ...  0.101892  0.187048  0.107720  0.282628  0.000801  1.224277e-03   \n5117   ...  0.086984  0.160138  0.080648  0.260979  0.002663  6.911051e-04   \n29587  ...  0.095345  0.176895  0.098732  0.274533  0.002290  1.187466e-03   \n2152   ...  0.118456  0.187616  0.098224  0.274575  0.003466  1.899352e-03   \n13238  ...  0.097031  0.177662  0.098667  0.272079  0.001946  0.000000e+00   \n2846   ...  0.176652  0.347165  0.311230  0.388000  0.011448  1.187466e-02   \n15526  ...  0.086779  0.160970  0.081132  0.261818  0.000572  4.702365e-04   \n17264  ...  0.177521  0.160138  0.080648  0.260979  0.008074  3.871139e-03   \n13526  ...  0.121277  0.220827  0.142971  0.310102  0.000000  1.781199e-03   \n10837  ...  0.119485  0.196222  0.115868  0.288149  0.002743  1.252776e-03   \n9951   ...  0.086330  0.165265  0.080635  0.260969  0.013672  0.000000e+00   \n7289   ...  0.097424  0.177847  0.099984  0.260979  0.004579  8.365697e-04   \n...    ...       ...       ...       ...       ...       ...           ...   \n23607  ...  0.179862  0.323133  0.254845  0.399080  0.008013  4.156130e-03   \n9404   ...  0.145640  0.234753  0.157872  0.322940  0.006869  2.384431e-03   \n28289  ...  0.109108  0.194898  0.098991  0.274999  0.002290  1.187466e-03   \n8763   ...  0.093409  0.313131  0.202046  0.405874  0.030936  7.676967e-03   \n7288   ...  0.086345  0.160138  0.080648  0.260979  0.000000  0.000000e+00   \n27861  ...  0.139561  0.254712  0.181264  0.340773  0.005724  2.621331e-03   \n27975  ...  0.090587  0.171715  0.081917  0.260979  0.003883  4.600836e-03   \n17598  ...  0.100592  0.186964  0.108168  0.282639  0.002061  8.905994e-04   \n2766   ...  0.130908  0.228397  0.152228  0.315320  0.006897  2.968665e-03   \n7679   ...  0.100330  0.184495  0.106724  0.280748  0.001832  7.124795e-04   \n13743  ...  0.087716  0.162490  0.083124  0.262898  0.000000  0.000000e+00   \n24216  ...  0.086947  0.160367  0.080968  0.261098  0.001442  6.554811e-04   \n9734   ...  0.086619  0.160138  0.080648  0.260979  0.003604  2.974602e-04   \n15095  ...  0.086345  0.160138  0.080648  0.260979  0.000000  0.000000e+00   \n1729   ...  0.106404  0.193721  0.116768  0.289542  0.002290  8.905994e-04   \n6150   ...  0.088944  0.165965  0.080763  0.272067  0.001969  2.045214e-01   \n11064  ...  0.200474  0.322736  0.245947  0.388694  0.000000  4.220847e-03   \n23387  ...  0.086757  0.174615  0.088427  0.266194  0.000000  8.917868e-04   \n16862  ...  0.086763  0.164948  0.081342  0.262027  0.007120  4.524245e-04   \n9387   ...  0.109240  0.198476  0.105653  0.268156  0.002067  8.680375e-04   \n1268   ...  0.096060  0.177000  0.100091  0.275690  0.001145  9.499727e-04   \n12402  ...  0.097075  0.176302  0.095341  0.270740  0.002090  8.721936e-04   \n2873   ...  0.111836  0.201025  0.123083  0.294923  0.005724  1.543706e-03   \n9212   ...  0.121652  0.221411  0.146591  0.318123  0.003434  0.000000e+00   \n8612   ...  0.102650  0.189487  0.112095  0.284414  0.001710  8.704124e-04   \n7842   ...  0.086345  0.160138  0.080648  0.260979  0.000000  0.000000e+00   \n3194   ...  0.089566  0.165314  0.085731  0.265512  0.001421  1.009346e-03   \n14705  ...  0.086345  0.160138  0.080648  0.260979  0.001358  0.000000e+00   \n24390  ...  0.091395  0.166618  0.087344  0.265159  0.001511  1.187466e-03   \n444    ...  0.215126  0.349141  0.273419  0.413797  0.013767  5.349534e-03   \n\n            X20       X21       X22       X23  \n17808  0.000000  0.000000  0.000000  0.000000  \n9043   0.004910  0.000000  0.007291  0.000000  \n27447  0.004258  0.000000  0.022006  0.001366  \n27111  0.000000  0.001932  0.000000  0.002837  \n11739  0.000000  0.000000  0.000000  0.000000  \n10483  0.001228  0.001771  0.002345  0.001892  \n3459   0.005547  0.007902  0.011572  0.009503  \n11242  0.006473  0.008696  0.011723  0.000000  \n8740   0.004399  0.003221  0.004689  0.003878  \n28426  0.002009  0.004348  0.004689  0.003216  \n27338  0.019157  0.000000  0.043085  0.003327  \n17132  0.002245  0.001757  0.002359  0.004759  \n6751   0.000811  0.001225  0.005861  0.003405  \n5426   0.000221  0.001929  0.026254  0.055532  \n3061   0.001339  0.004171  0.000000  0.001892  \n9246   0.000930  0.001341  0.001953  0.001576  \n2337   0.011574  0.000000  0.003123  0.000000  \n2967   0.001287  0.000005  0.004124  0.007725  \n13904  0.001081  0.001127  0.003517  0.002837  \n5117   0.000000  0.000000  0.000000  0.054223  \n29587  0.002232  0.001610  0.001876  0.001324  \n2152   0.001414  0.001034  0.001526  0.003149  \n13238  0.000781  0.000998  0.001876  0.000567  \n2846   0.111656  0.096618  0.000000  0.009458  \n15526  0.000545  0.000000  0.002345  0.000000  \n17264  0.000000  0.000000  0.000000  0.000000  \n13526  0.002685  0.003915  0.005704  0.004604  \n10837  0.001420  0.002024  0.002919  0.002153  \n9951   0.006105  0.000000  0.000000  0.000000  \n7289   0.000000  0.001610  0.000000  0.000000  \n...         ...       ...       ...       ...  \n23607  0.007812  0.011272  0.016412  0.013241  \n9404   0.003348  0.004831  0.009378  0.007566  \n28289  0.002332  0.001610  0.001519  0.003783  \n8763   0.181280  0.198135  0.442444  0.378848  \n7288   0.000000  0.000000  0.000000  0.005675  \n27861  0.006696  0.005853  0.009378  0.009458  \n27975  0.016215  0.002066  0.000000  0.000000  \n17598  0.003348  0.000000  0.002347  0.004729  \n2766   0.007589  0.008052  0.011723  0.009458  \n7679   0.001228  0.001610  0.000000  0.003685  \n13743  0.000000  0.000000  0.000000  0.000000  \n24216  0.000271  0.000520  0.000363  0.000395  \n9734   0.000000  0.000000  0.000000  0.000000  \n15095  0.000000  0.000000  0.000000  0.000000  \n1729   0.000000  0.002415  0.003517  0.005675  \n6150   0.007613  0.000079  0.035168  0.022699  \n11064  0.006752  0.009662  0.021361  0.011673  \n23387  0.017152  0.000322  0.000000  0.000000  \n16862  0.006480  0.000000  0.003198  0.000946  \n9387   0.002232  0.000000  0.004689  0.000083  \n1268   0.000893  0.003221  0.000000  0.003026  \n12402  0.000574  0.000936  0.001435  0.000590  \n2873   0.001674  0.002415  0.004689  0.002837  \n9212   0.002678  0.004026  0.021101  0.000000  \n8612   0.002791  0.002833  0.000000  0.030098  \n7842   0.000000  0.000000  0.000000  0.000000  \n3194   0.000000  0.000000  0.002345  0.000000  \n14705  0.000000  0.000000  0.000000  0.000000  \n24390  0.001433  0.000425  0.000256  0.000738  \n444    0.008259  0.012882  0.018756  0.013241  \n\n[13200 rows x 23 columns]\n17808    1.0\n9043     1.0\n27447    0.0\n27111    1.0\n11739    1.0\n10483    0.0\n3459     1.0\n11242    0.0\n8740     0.0\n28426    0.0\n27338    0.0\n17132    1.0\n6751     1.0\n5426     1.0\n3061     0.0\n9246     1.0\n2337     0.0\n2967     1.0\n13904    1.0\n5117     1.0\n29587    0.0\n2152     0.0\n13238    1.0\n2846     0.0\n15526    1.0\n17264    1.0\n13526    1.0\n10837    0.0\n9951     1.0\n7289     0.0\n        ... \n23607    1.0\n9404     0.0\n28289    1.0\n8763     0.0\n7288     0.0\n27861    0.0\n27975    0.0\n17598    1.0\n2766     0.0\n7679     1.0\n13743    1.0\n24216    1.0\n9734     0.0\n15095    1.0\n1729     0.0\n6150     1.0\n11064    0.0\n23387    1.0\n16862    1.0\n9387     0.0\n1268     1.0\n12402    1.0\n2873     0.0\n9212     0.0\n8612     0.0\n7842     1.0\n3194     1.0\n14705    1.0\n24390    1.0\n444      1.0\nName: Y, Length: 13200, dtype: float64\n"
    }
   ],
   "source": [
    "print(X)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.reshape(X_train.values,(len(X_train),23,1))\n",
    "X_test = np.reshape(X_test.values,(len(X_test),23,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2640\n"
    }
   ],
   "source": [
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.17171717 1.         0.33333333 ... 0.4        0.4        0.4       ]\n [0.01010101 0.         0.33333333 ... 0.         0.         0.        ]\n [0.04040404 1.         0.33333333 ... 0.1        0.4        0.2       ]\n ...\n [0.6969697  1.         0.33333333 ... 0.2        0.2        0.2       ]\n [0.07070707 1.         0.5        ... 0.2        0.2        0.2       ]\n [0.14141414 0.         0.33333333 ... 0.2        0.2        0.2       ]]\n"
    },
    {
     "data": {
      "text/plain": "array([[0.17171717, 1.        , 0.33333333, ..., 0.01932367, 0.        ,\n        0.02232033],\n       [0.01010101, 0.        , 0.33333333, ..., 0.00483092, 0.00182872,\n        0.        ],\n       [0.04040404, 1.        , 0.33333333, ..., 0.        , 0.00163881,\n        0.0010971 ],\n       ...,\n       [0.6969697 , 1.        , 0.33333333, ..., 0.0016248 , 0.00234451,\n        0.00189155],\n       [0.07070707, 1.        , 0.5       , ..., 0.01288245, 0.00703352,\n        0.00605297],\n       [0.14141414, 0.        , 0.33333333, ..., 0.00805153, 0.00937803,\n        0.00945777]])"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "poly = PolynomialFeatures(16)\n",
    "\n",
    "temp_x_train = np.reshape(X_train,(len(X_train),23))\n",
    "temp_y_train = np.reshape(y_train,(len(y_train)))\n",
    "\n",
    "temp_x_test = np.reshape(X_test,(len(X_test),23))\n",
    "temp_y_test = np.reshape(y_test,(len(y_test)))\n",
    "\n",
    "sel = VarianceThreshold(threshold=.01)\n",
    "\n",
    "sel.fit(temp_x_train)\n",
    "\n",
    "\n",
    "print(sel.transform(temp_x_train))\n",
    "temp_x_train  \n",
    "\n",
    "reg = LogisticRegression().fit(poly.fit_transform(temp_x_train), temp_y_train)\n",
    "\n",
    "print(len(reg.coef_[0]))\n",
    "reg.intercept_\n",
    "y_predict= reg.predict(poly.fit_transform(temp_x_test))\n",
    "print(reg.score(poly.fit_transform(temp_x_train),temp_y_train))\n",
    "reg.score(poly.fit_transform(temp_x_test),temp_y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(temp_y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_x_train = np.reshape(X_train,(len(X_train),23))\n",
    "temp_y_train = np.reshape(y_train,(len(y_train)))\n",
    "\n",
    "temp_x_test = np.reshape(X_test,(len(X_test),23))\n",
    "temp_y_test = np.reshape(y_test,(len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0.6848484848484848\n"
    },
    {
     "data": {
      "text/plain": "-0.31517038525001784"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "m= RFECV(RandomForestClassifier(),scoring=\"accuracy\")\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "\n",
    "reg = LogisticRegression().fit(poly.fit_transform(temp_x_train), temp_y_train)\n",
    "m.fit(poly.fit_transform(temp_x_train),temp_y_train)\n",
    "m.predict(poly.fit_transform(temp_x_test))\n",
    "print(m.score(poly.fit_transform(temp_x_test),temp_y_test))\n",
    "\n",
    "# print(len(reg.coef_[0]))\n",
    "# reg.intercept_\n",
    "# y_predict= reg.predict(poly.fit_transform(temp_x_test))\n",
    "# print(reg.score(poly.fit_transform(temp_x_train),temp_y_train))\n",
    "# reg.score(poly.fit_transform(temp_x_test),temp_y_test)\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(temp_y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.regression.linear_model as sm\n",
    "def backwardElimination(x, Y, sl):\n",
    "    x = np.array(x, dtype=float)\n",
    "    numVars = len(x[0])\n",
    "    for i in range(0, numVars):\n",
    "        regressor_OLS = sm.OLS(Y, x).fit()\n",
    "        maxVar = max(regressor_OLS.pvalues)\n",
    "        if maxVar > sl:\n",
    "            for j in range(0, numVars - i):\n",
    "                if (regressor_OLS.pvalues[j].astype(float) == maxVar):\n",
    "                    x = np.delete(x, j, 1)\n",
    "                    \n",
    "    regressor_OLS.summary()\n",
    "    return x\n",
    "\n",
    "SL = 0.05\n",
    "data_modeled = backwardElimination(X, y,SL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[[0.24242424 0.         0.5        ... 0.         0.         0.        ]\n [0.33333333 1.         0.33333333 ... 0.         0.         0.        ]\n [0.01010101 0.         0.33333333 ... 0.4        0.4        0.4       ]\n ...\n [0.04040404 1.         0.33333333 ... 0.2        0.         0.        ]\n [0.04040404 1.         0.33333333 ... 0.2        0.2        0.2       ]\n [0.21212121 1.         0.16666667 ... 0.1        0.1        0.1       ]]\n300\n0.696685606060606\n0.6848484848484848\n"
    },
    {
     "data": {
      "text/plain": "-0.26060678409480276"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2)\n",
    "\n",
    "import numpy as np\n",
    "X_train = np.reshape(X_train.values,(len(X_train),23,1))\n",
    "X_test = np.reshape(X_test.values,(len(X_test),23,1))\n",
    "\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "poly = PolynomialFeatures(2)\n",
    "\n",
    "temp_x_train = np.reshape(X_train,(len(X_train),23))\n",
    "temp_y_train = np.reshape(y_train,(len(y_train)))\n",
    "\n",
    "temp_x_test = np.reshape(X_test,(len(X_test),23))\n",
    "temp_y_test = np.reshape(y_test,(len(y_test)))\n",
    "\n",
    "sel = VarianceThreshold(threshold=.01)\n",
    "\n",
    "sel.fit(temp_x_train)\n",
    "\n",
    "\n",
    "print(sel.transform(temp_x_train))\n",
    "temp_x_train  \n",
    "\n",
    "reg = LogisticRegression().fit(poly.fit_transform(temp_x_train), temp_y_train)\n",
    "\n",
    "print(len(reg.coef_[0]))\n",
    "reg.intercept_\n",
    "y_predict= reg.predict(poly.fit_transform(temp_x_test))\n",
    "print(reg.score(poly.fit_transform(temp_x_train),temp_y_train))\n",
    "print(reg.score(poly.fit_transform(temp_x_test),temp_y_test))\n",
    "\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "r2_score(temp_y_test,y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "s: 0.6010 - accuracy: 0.6965\n2640/2640 [==============================] - 0s 34us/sample - loss: 0.6218 - accuracy: 0.6784\n0.6784091\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6004 - accuracy: 0.6987\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6029 - accuracy: 0.6989\n0.6988636\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.5987 - accuracy: 0.6977\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6055 - accuracy: 0.6977\n0.69772726\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.5983 - accuracy: 0.6958\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6129 - accuracy: 0.6883\n0.6882576\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.5972 - accuracy: 0.6976\n2640/2640 [==============================] - 0s 42us/sample - loss: 0.6050 - accuracy: 0.6936\n0.6935606\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 65us/sample - loss: 0.5968 - accuracy: 0.6991\n2640/2640 [==============================] - 0s 42us/sample - loss: 0.6040 - accuracy: 0.6951\n0.69507575\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 57us/sample - loss: 0.5958 - accuracy: 0.6966\n2640/2640 [==============================] - 0s 35us/sample - loss: 0.6017 - accuracy: 0.6962\n0.6962121\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.5947 - accuracy: 0.6989\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6023 - accuracy: 0.6955\n0.69545454\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.5941 - accuracy: 0.6986\n2640/2640 [==============================] - 0s 33us/sample - loss: 0.6012 - accuracy: 0.6913\n0.6912879\n(10560, 22, 1)\nAbout to start estimator\nOutput layer\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 149us/sample - loss: 1.2140 - accuracy: 0.6282\n2640/2640 [==============================] - 0s 104us/sample - loss: 0.6654 - accuracy: 0.6314\n0.6314394\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.6356 - accuracy: 0.6714\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6377 - accuracy: 0.6716\n0.6715909\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 64us/sample - loss: 0.6230 - accuracy: 0.6809\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6301 - accuracy: 0.6413\n0.64128786\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6188 - accuracy: 0.6840\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.6276 - accuracy: 0.6792\n0.6791667\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6151 - accuracy: 0.6884\n2640/2640 [==============================] - 0s 47us/sample - loss: 0.6185 - accuracy: 0.6860\n0.68598485\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.6135 - accuracy: 0.6878\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.6124 - accuracy: 0.6928\n0.692803\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 57us/sample - loss: 0.6095 - accuracy: 0.6893\n2640/2640 [==============================] - 0s 47us/sample - loss: 0.6110 - accuracy: 0.6924\n0.69242424\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 64us/sample - loss: 0.6060 - accuracy: 0.6930\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6235 - accuracy: 0.6636\n0.6636364\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.6041 - accuracy: 0.6949\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6072 - accuracy: 0.6951\n0.69507575\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6030 - accuracy: 0.6940\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6056 - accuracy: 0.6951\n0.69507575\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6008 - accuracy: 0.6972\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6228 - accuracy: 0.6723\n0.6723485\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.5996 - accuracy: 0.6974\n2640/2640 [==============================] - 0s 39us/sample - loss: 0.6374 - accuracy: 0.6739\n0.67386365\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 57us/sample - loss: 0.5987 - accuracy: 0.6983\n2640/2640 [==============================] - 0s 39us/sample - loss: 0.6034 - accuracy: 0.6936\n0.6935606\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 64us/sample - loss: 0.5965 - accuracy: 0.6966\n2640/2640 [==============================] - 0s 54us/sample - loss: 0.6076 - accuracy: 0.6894\n0.68939394\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.5961 - accuracy: 0.6967\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6053 - accuracy: 0.6966\n0.6965909\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.5945 - accuracy: 0.6986\n2640/2640 [==============================] - 0s 33us/sample - loss: 0.6016 - accuracy: 0.6943\n0.6943182\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.5934 - accuracy: 0.7005\n2640/2640 [==============================] - 0s 34us/sample - loss: 0.6018 - accuracy: 0.6894\n0.68939394\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.5924 - accuracy: 0.7016\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6004 - accuracy: 0.6966\n0.6965909\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.5915 - accuracy: 0.7005\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6122 - accuracy: 0.6792\n0.6791667\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 62us/sample - loss: 0.5904 - accuracy: 0.7023\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.5963 - accuracy: 0.6985\n0.69848484\n(10560, 22, 1)\nAbout to start estimator\nOutput layer\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 149us/sample - loss: 1.2126 - accuracy: 0.6325\n2640/2640 [==============================] - 0s 101us/sample - loss: 0.6621 - accuracy: 0.6640\n0.6640152\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 64us/sample - loss: 0.6354 - accuracy: 0.6754\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6292 - accuracy: 0.6811\n0.6810606\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6229 - accuracy: 0.6798\n2640/2640 [==============================] - 0s 41us/sample - loss: 0.6385 - accuracy: 0.6360\n0.63598484\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.6189 - accuracy: 0.6848\n2640/2640 [==============================] - 0s 49us/sample - loss: 0.6173 - accuracy: 0.6890\n0.68901515\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 67us/sample - loss: 0.6151 - accuracy: 0.6897\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6173 - accuracy: 0.6902\n0.6901515\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6136 - accuracy: 0.6871\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6145 - accuracy: 0.6883\n0.6882576\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.6110 - accuracy: 0.6881\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6138 - accuracy: 0.6860\n0.68598485\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 67us/sample - loss: 0.6102 - accuracy: 0.6909\n2640/2640 [==============================] - 0s 42us/sample - loss: 0.6100 - accuracy: 0.6894\n0.68939394\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.6074 - accuracy: 0.6895\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.6077 - accuracy: 0.6905\n0.6905303\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 62us/sample - loss: 0.6053 - accuracy: 0.6902\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6070 - accuracy: 0.6875\n0.6875\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 80us/sample - loss: 0.6024 - accuracy: 0.6926\n2640/2640 [==============================] - 0s 56us/sample - loss: 0.6087 - accuracy: 0.6955\n0.69545454\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.6021 - accuracy: 0.6930\n2640/2640 [==============================] - 0s 56us/sample - loss: 0.6207 - accuracy: 0.6758\n0.6757576\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.6008 - accuracy: 0.6932\n2640/2640 [==============================] - 0s 43us/sample - loss: 0.6072 - accuracy: 0.6951\n0.69507575\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.6002 - accuracy: 0.6968\n2640/2640 [==============================] - 0s 56us/sample - loss: 0.6037 - accuracy: 0.6928\n0.692803\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.5989 - accuracy: 0.6952\n2640/2640 [==============================] - 0s 41us/sample - loss: 0.6027 - accuracy: 0.6970\n0.6969697\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.5987 - accuracy: 0.6979\n2640/2640 [==============================] - 0s 52us/sample - loss: 0.6112 - accuracy: 0.6879\n0.6878788\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 65us/sample - loss: 0.5980 - accuracy: 0.6967\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6029 - accuracy: 0.6898\n0.6897727\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 65us/sample - loss: 0.5964 - accuracy: 0.6974\n2640/2640 [==============================] - 0s 45us/sample - loss: 0.6025 - accuracy: 0.6962\n0.6962121\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 65us/sample - loss: 0.5968 - accuracy: 0.6953\n2640/2640 [==============================] - 0s 42us/sample - loss: 0.6006 - accuracy: 0.6970\n0.6969697\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.5956 - accuracy: 0.6992\n2640/2640 [==============================] - 0s 45us/sample - loss: 0.6023 - accuracy: 0.6943\n0.6943182\n(10560, 22, 1)\nAbout to start estimator\nOutput layer\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 160us/sample - loss: 1.2156 - accuracy: 0.6234\n2640/2640 [==============================] - 0s 99us/sample - loss: 0.6668 - accuracy: 0.6644\n0.66439396\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 66us/sample - loss: 0.6385 - accuracy: 0.6687\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6297 - accuracy: 0.6701\n0.6700758\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 57us/sample - loss: 0.6249 - accuracy: 0.6803\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6250 - accuracy: 0.6848\n0.6848485\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.6200 - accuracy: 0.6829\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6210 - accuracy: 0.6848\n0.6848485\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.6163 - accuracy: 0.6869\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6185 - accuracy: 0.6905\n0.6905303\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6135 - accuracy: 0.6906\n2640/2640 [==============================] - 0s 48us/sample - loss: 0.6160 - accuracy: 0.6879\n0.6878788\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.6130 - accuracy: 0.6886\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.6253 - accuracy: 0.6591\n0.65909094\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6104 - accuracy: 0.6889\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6115 - accuracy: 0.6905\n0.6905303\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6092 - accuracy: 0.6910\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6110 - accuracy: 0.6939\n0.6939394\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 64us/sample - loss: 0.6059 - accuracy: 0.6953\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6083 - accuracy: 0.6902\n0.6901515\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 57us/sample - loss: 0.6038 - accuracy: 0.6937\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6101 - accuracy: 0.6917\n0.69166666\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.6020 - accuracy: 0.6956\n2640/2640 [==============================] - 0s 49us/sample - loss: 0.6178 - accuracy: 0.6852\n0.6852273\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 65us/sample - loss: 0.6011 - accuracy: 0.6934\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6055 - accuracy: 0.6939\n0.6939394\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.6001 - accuracy: 0.6981\n2640/2640 [==============================] - 0s 31us/sample - loss: 0.6034 - accuracy: 0.6936\n0.6935606\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 57us/sample - loss: 0.5989 - accuracy: 0.6993\n2640/2640 [==============================] - 0s 35us/sample - loss: 0.6048 - accuracy: 0.6909\n0.6909091\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 62us/sample - loss: 0.5984 - accuracy: 0.6967\n2640/2640 [==============================] - 0s 31us/sample - loss: 0.6165 - accuracy: 0.6765\n0.67651516\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.5981 - accuracy: 0.6975\n2640/2640 [==============================] - 0s 32us/sample - loss: 0.6101 - accuracy: 0.6936\n0.6935606\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 62us/sample - loss: 0.5974 - accuracy: 0.6975\n2640/2640 [==============================] - 0s 35us/sample - loss: 0.6019 - accuracy: 0.6966\n0.6965909\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.5966 - accuracy: 0.6977\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6017 - accuracy: 0.6936\n0.6935606\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.5952 - accuracy: 0.6985\n2640/2640 [==============================] - 0s 41us/sample - loss: 0.6002 - accuracy: 0.6943\n0.6943182\n(10560, 22, 1)\nAbout to start estimator\nOutput layer\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 147us/sample - loss: 1.2136 - accuracy: 0.6350\n2640/2640 [==============================] - 0s 103us/sample - loss: 0.6749 - accuracy: 0.6508\n0.65075755\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6365 - accuracy: 0.6716\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6437 - accuracy: 0.6591\n0.65909094\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6245 - accuracy: 0.6804\n2640/2640 [==============================] - 0s 47us/sample - loss: 0.6358 - accuracy: 0.6705\n0.67045456\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6189 - accuracy: 0.6848\n2640/2640 [==============================] - 0s 37us/sample - loss: 0.6275 - accuracy: 0.6761\n0.6761364\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.6162 - accuracy: 0.6882\n2640/2640 [==============================] - 0s 42us/sample - loss: 0.6167 - accuracy: 0.6879\n0.6878788\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 63us/sample - loss: 0.6131 - accuracy: 0.6884\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6409 - accuracy: 0.6292\n0.62916666\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6117 - accuracy: 0.6898\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6253 - accuracy: 0.6515\n0.6515151\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 62us/sample - loss: 0.6094 - accuracy: 0.6890\n2640/2640 [==============================] - 0s 32us/sample - loss: 0.6100 - accuracy: 0.6883\n0.6882576\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.6077 - accuracy: 0.6908\n2640/2640 [==============================] - 0s 34us/sample - loss: 0.6077 - accuracy: 0.6883\n0.6882576\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.6041 - accuracy: 0.6924\n2640/2640 [==============================] - 0s 41us/sample - loss: 0.6069 - accuracy: 0.6913\n0.6912879\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.6019 - accuracy: 0.6952\n2640/2640 [==============================] - 0s 51us/sample - loss: 0.6074 - accuracy: 0.6890\n0.68901515\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 58us/sample - loss: 0.6010 - accuracy: 0.6953\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.6058 - accuracy: 0.6977\n0.69772726\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.5995 - accuracy: 0.6985\n2640/2640 [==============================] - 0s 36us/sample - loss: 0.6118 - accuracy: 0.6905\n0.6905303\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 61us/sample - loss: 0.5983 - accuracy: 0.6965\n2640/2640 [==============================] - 0s 33us/sample - loss: 0.6096 - accuracy: 0.6928\n0.692803\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 62us/sample - loss: 0.5971 - accuracy: 0.6984\n2640/2640 [==============================] - 0s 34us/sample - loss: 0.6086 - accuracy: 0.6951\n0.69507575\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 67us/sample - loss: 0.5951 - accuracy: 0.6981\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.6183 - accuracy: 0.6701\n0.6700758\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 59us/sample - loss: 0.5933 - accuracy: 0.6984\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.6024 - accuracy: 0.6966\n0.6965909\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.5924 - accuracy: 0.7000\n2640/2640 [==============================] - 0s 38us/sample - loss: 0.5997 - accuracy: 0.6947\n0.69469696\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 60us/sample - loss: 0.5913 - accuracy: 0.7004\n2640/2640 [==============================] - 0s 44us/sample - loss: 0.5994 - accuracy: 0.6989\n0.6988636\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 56us/sample - loss: 0.5899 - accuracy: 0.6992\n2640/2640 [==============================] - 0s 40us/sample - loss: 0.5991 - accuracy: 0.6966\n0.6965909\n"
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Dense,Dropout, Conv1D, MaxPooling1D, GlobalMaxPooling1D,Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print(\"Import Worked\")\n",
    "\n",
    "global_max=0\n",
    "the_needed_index=0\n",
    "for counter in range(23):\n",
    "    filtered_x_train=X_train\n",
    "    print(np.delete(filtered_x_train,counter,axis=1).shape)\n",
    "\n",
    "\n",
    "    def create_model():\n",
    "        model=Sequential()\n",
    "        model.add(Conv1D(250, kernel_size=21, activation=\"relu\", input_shape=(len(filtered_x_train[0]),1)))\n",
    "        # model.add(Conv1D(filters=250, kernel_size=3, activation=\"relu\"))\n",
    "        model.add(MaxPooling1D(pool_size=2))\n",
    "        # model.add(GlobalMaxPooling1D())\n",
    "        model.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "        model.add(Dense(1,activation=\"sigmoid\"))\n",
    "        \n",
    "        #COMPILE MODE\n",
    "        print(\"Output layer\")\n",
    "        model.compile(loss='binary_crossentropy', optimizer='nadam',metrics=[\"accuracy\"])\n",
    "        return model\n",
    "    \n",
    "    from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "    print(\"About to start estimator\")   \n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    local_max=0\n",
    "    for i in range(20):\n",
    "        model.fit(X_train,y_train,epochs=1, batch_size=50,verbose=1)\n",
    "        aasdsd,temp=model.evaluate(X_test,y_test)\n",
    "        print(temp)\n",
    "        local_max=max(local_max,temp)\n",
    "    if(local_max>global_max):\n",
    "        global_max=local_max\n",
    "        print(global_max)\n",
    "        the_needed_index=counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "14\n"
    }
   ],
   "source": [
    "print(the_needed_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "0560 [==============================] - 1s 68us/sample - loss: 0.1979 - accuracy: 0.9136\nEpoch 1824/2000\n10560/10560 [==============================] - 1s 76us/sample - loss: 0.1804 - accuracy: 0.9201\nEpoch 1825/2000\n10560/10560 [==============================] - 1s 76us/sample - loss: 0.1578 - accuracy: 0.9278\nEpoch 1826/2000\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.1587 - accuracy: 0.9270\nEpoch 1827/2000\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.1545 - accuracy: 0.9293\nEpoch 1828/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1684 - accuracy: 0.9216\nEpoch 1829/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1698 - accuracy: 0.9212\nEpoch 1830/2000\n10560/10560 [==============================] - 1s 80us/sample - loss: 0.1710 - accuracy: 0.9228\nEpoch 1831/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1595 - accuracy: 0.9260\nEpoch 1832/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1741 - accuracy: 0.9210\nEpoch 1833/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1832 - accuracy: 0.9183\nEpoch 1834/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1758 - accuracy: 0.9215\nEpoch 1835/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1707 - accuracy: 0.9237\nEpoch 1836/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1692 - accuracy: 0.9220\nEpoch 1837/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1669 - accuracy: 0.9246\nEpoch 1838/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1679 - accuracy: 0.9239\nEpoch 1839/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1895 - accuracy: 0.9157\nEpoch 1840/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1559 - accuracy: 0.9280\nEpoch 1841/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1586 - accuracy: 0.9277\nEpoch 1842/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1558 - accuracy: 0.9287\nEpoch 1843/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1549 - accuracy: 0.9281\nEpoch 1844/2000\n10560/10560 [==============================] - 1s 76us/sample - loss: 0.1496 - accuracy: 0.9319\nEpoch 1845/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.2410 - accuracy: 0.9017\nEpoch 1846/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1735 - accuracy: 0.9201\nEpoch 1847/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1649 - accuracy: 0.9239\nEpoch 1848/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1794 - accuracy: 0.9213\nEpoch 1849/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1524 - accuracy: 0.9301\nEpoch 1850/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1511 - accuracy: 0.9307\nEpoch 1851/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1579 - accuracy: 0.9277\nEpoch 1852/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1727 - accuracy: 0.9187\nEpoch 1853/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1604 - accuracy: 0.9259\nEpoch 1854/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1607 - accuracy: 0.9259\nEpoch 1855/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1569 - accuracy: 0.9266\nEpoch 1856/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1717 - accuracy: 0.9200\nEpoch 1857/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1699 - accuracy: 0.9235\nEpoch 1858/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1707 - accuracy: 0.9218\nEpoch 1859/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1693 - accuracy: 0.9217\nEpoch 1860/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1543 - accuracy: 0.9287\nEpoch 1861/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1578 - accuracy: 0.9264\nEpoch 1862/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1872 - accuracy: 0.9136\nEpoch 1863/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1892 - accuracy: 0.9152\nEpoch 1864/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1914 - accuracy: 0.9172\nEpoch 1865/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1694 - accuracy: 0.9226\nEpoch 1866/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1533 - accuracy: 0.9295\nEpoch 1867/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1550 - accuracy: 0.9279\nEpoch 1868/2000\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.1503 - accuracy: 0.9304\nEpoch 1869/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1660 - accuracy: 0.9259\nEpoch 1870/2000\n10560/10560 [==============================] - 1s 87us/sample - loss: 0.2127 - accuracy: 0.9119\nEpoch 1871/2000\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.1973 - accuracy: 0.9127\nEpoch 1872/2000\n10560/10560 [==============================] - 1s 83us/sample - loss: 0.1488 - accuracy: 0.9329\nEpoch 1873/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1576 - accuracy: 0.9280\nEpoch 1874/2000\n10250/10560 [============================>.] - ETA: 0s - loss: 0.1597 - accuracy: 0.9210560/10560 [==============================] - 1s 71us/sample - loss: 0.1589 - accuracy: 0.9284\nEpoch 1875/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1532 - accuracy: 0.9279\nEpoch 1876/2000\n10560/10560 [==============================] - 1s 80us/sample - loss: 0.1635 - accuracy: 0.9253\nEpoch 1877/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1532 - accuracy: 0.9286\nEpoch 1878/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1706 - accuracy: 0.9220\nEpoch 1879/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1700 - accuracy: 0.9215\nEpoch 1880/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1572 - accuracy: 0.9275\nEpoch 1881/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1974 - accuracy: 0.9136\nEpoch 1882/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1712 - accuracy: 0.9217\nEpoch 1883/2000\n10560/10560 [==============================] - 1s 78us/sample - loss: 0.1559 - accuracy: 0.9273\nEpoch 1884/2000\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.1579 - accuracy: 0.9294\nEpoch 1885/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1596 - accuracy: 0.9255\nEpoch 1886/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.2059 - accuracy: 0.9096\nEpoch 1887/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1610 - accuracy: 0.9286\nEpoch 1888/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1654 - accuracy: 0.9230\nEpoch 1889/2000\n10560/10560 [==============================] - 1s 76us/sample - loss: 0.1947 - accuracy: 0.9126\nEpoch 1890/2000\n10560/10560 [==============================] - 1s 76us/sample - loss: 0.1873 - accuracy: 0.9157\nEpoch 1891/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1967 - accuracy: 0.9117\nEpoch 1892/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1542 - accuracy: 0.9284\nEpoch 1893/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1505 - accuracy: 0.9303\nEpoch 1894/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1536 - accuracy: 0.9314\nEpoch 1895/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1475 - accuracy: 0.9295\nEpoch 1896/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1606 - accuracy: 0.9269\nEpoch 1897/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1649 - accuracy: 0.9226\nEpoch 1898/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1593 - accuracy: 0.9262\nEpoch 1899/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1909 - accuracy: 0.9114\nEpoch 1900/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1845 - accuracy: 0.9187\nEpoch 1901/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.2010 - accuracy: 0.9140\nEpoch 1902/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1556 - accuracy: 0.9277\nEpoch 1903/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1539 - accuracy: 0.9277\nEpoch 1904/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1521 - accuracy: 0.9315\nEpoch 1905/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1522 - accuracy: 0.9292\nEpoch 1906/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1450 - accuracy: 0.9321\nEpoch 1907/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1506 - accuracy: 0.9291\nEpoch 1908/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1556 - accuracy: 0.9272\nEpoch 1909/2000\n10560/10560 [==============================] - 1s 78us/sample - loss: 0.1474 - accuracy: 0.9310\nEpoch 1910/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1761 - accuracy: 0.9178\nEpoch 1911/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1872 - accuracy: 0.9161\nEpoch 1912/2000\n10560/10560 [==============================] - 1s 78us/sample - loss: 0.2007 - accuracy: 0.9136\nEpoch 1913/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1891 - accuracy: 0.9175\nEpoch 1914/2000\n10560/10560 [==============================] - 1s 78us/sample - loss: 0.1494 - accuracy: 0.9314\nEpoch 1915/2000\n10560/10560 [==============================] - 1s 84us/sample - loss: 0.1507 - accuracy: 0.9314\nEpoch 1916/2000\n10560/10560 [==============================] - 1s 80us/sample - loss: 0.1464 - accuracy: 0.9320\nEpoch 1917/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1566 - accuracy: 0.9240\nEpoch 1918/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1684 - accuracy: 0.9228\nEpoch 1919/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1497 - accuracy: 0.9296\nEpoch 1920/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1693 - accuracy: 0.9248\nEpoch 1921/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1607 - accuracy: 0.9247\nEpoch 1922/2000\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.1628 - accuracy: 0.9289\nEpoch 1923/2000\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.1848 - accuracy: 0.9172\nEpoch 1924/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1717 - accuracy: 0.9219\nEpoch 1925/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1542 - accuracy: 0.9314\nEpoch 1926/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1532 - accuracy: 0.9310\nEpoch 1927/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1721 - accuracy: 0.9243\nEpoch 1928/2000\n10560/10560 [==============================] - 1s 98us/sample - loss: 0.1774 - accuracy: 0.9204\nEpoch 1929/2000\n10560/10560 [==============================] - 1s 103us/sample - loss: 0.1599 - accuracy: 0.9259\nEpoch 1930/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1643 - accuracy: 0.9247\nEpoch 1931/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1801 - accuracy: 0.9164\nEpoch 1932/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1780 - accuracy: 0.9191\nEpoch 1933/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1498 - accuracy: 0.9316\nEpoch 1934/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1454 - accuracy: 0.9330\nEpoch 1935/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1468 - accuracy: 0.9305\nEpoch 1936/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1581 - accuracy: 0.9285\nEpoch 1937/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.2046 - accuracy: 0.9063\nEpoch 1938/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.2246 - accuracy: 0.9045\nEpoch 1939/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1589 - accuracy: 0.9269\nEpoch 1940/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1430 - accuracy: 0.9317\nEpoch 1941/2000\n10560/10560 [==============================] - 1s 82us/sample - loss: 0.1482 - accuracy: 0.9311\nEpoch 1942/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1485 - accuracy: 0.9321\nEpoch 1943/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1786 - accuracy: 0.9221\nEpoch 1944/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1695 - accuracy: 0.9270\nEpoch 1945/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1584 - accuracy: 0.9272\nEpoch 1946/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1601 - accuracy: 0.9259\nEpoch 1947/2000\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.1449 - accuracy: 0.9323\nEpoch 1948/2000\n10560/10560 [==============================] - 1s 74us/sample - loss: 0.1568 - accuracy: 0.9277\nEpoch 1949/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.2146 - accuracy: 0.9117\nEpoch 1950/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1736 - accuracy: 0.9199\nEpoch 1951/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1489 - accuracy: 0.9321\nEpoch 1952/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1600 - accuracy: 0.9236\nEpoch 1953/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1702 - accuracy: 0.9207\nEpoch 1954/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1454 - accuracy: 0.9338\nEpoch 1955/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1578 - accuracy: 0.9272\nEpoch 1956/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1575 - accuracy: 0.9249\nEpoch 1957/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.2062 - accuracy: 0.9080\nEpoch 1958/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1621 - accuracy: 0.9246\nEpoch 1959/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1510 - accuracy: 0.9314\nEpoch 1960/2000\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.1440 - accuracy: 0.9352\nEpoch 1961/2000\n10560/10560 [==============================] - 1s 80us/sample - loss: 0.1466 - accuracy: 0.9304\nEpoch 1962/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1533 - accuracy: 0.9300\nEpoch 1963/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1609 - accuracy: 0.9262\nEpoch 1964/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1696 - accuracy: 0.9219\nEpoch 1965/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1677 - accuracy: 0.9254\nEpoch 1966/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1889 - accuracy: 0.9154\nEpoch 1967/2000\n10560/10560 [==============================] - 1s 81us/sample - loss: 0.2171 - accuracy: 0.9094\nEpoch 1968/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1712 - accuracy: 0.9223\nEpoch 1969/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1444 - accuracy: 0.9330\nEpoch 1970/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1467 - accuracy: 0.9316\nEpoch 1971/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1399 - accuracy: 0.9372\nEpoch 1972/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1477 - accuracy: 0.9323\nEpoch 1973/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1534 - accuracy: 0.9293\nEpoch 1974/2000\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.1684 - accuracy: 0.9218\nEpoch 1975/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1631 - accuracy: 0.9258\nEpoch 1976/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.2054 - accuracy: 0.9132\nEpoch 1977/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1860 - accuracy: 0.9182\nEpoch 1978/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1554 - accuracy: 0.9301\nEpoch 1979/2000\n10560/10560 [==============================] - 1s 68us/sample - loss: 0.1474 - accuracy: 0.9343\nEpoch 1980/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1416 - accuracy: 0.9354\nEpoch 1981/2000\n10560/10560 [==============================] - 1s 72us/sample - loss: 0.1498 - accuracy: 0.9331\nEpoch 1982/2000\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.1501 - accuracy: 0.9295\nEpoch 1983/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1770 - accuracy: 0.9189\nEpoch 1984/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1465 - accuracy: 0.9330\nEpoch 1985/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1606 - accuracy: 0.9285\nEpoch 1986/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.2000 - accuracy: 0.9182\nEpoch 1987/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.2532 - accuracy: 0.8940\nEpoch 1988/2000\n10560/10560 [==============================] - 1s 73us/sample - loss: 0.1724 - accuracy: 0.9226\nEpoch 1989/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1502 - accuracy: 0.9301\nEpoch 1990/2000\n10560/10560 [==============================] - 1s 71us/sample - loss: 0.1567 - accuracy: 0.9311\nEpoch 1991/2000\n10560/10560 [==============================] - 1s 69us/sample - loss: 0.1559 - accuracy: 0.9324\nEpoch 1992/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1475 - accuracy: 0.9305\nEpoch 1993/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1593 - accuracy: 0.9291\nEpoch 1994/2000\n10560/10560 [==============================] - 1s 76us/sample - loss: 0.1427 - accuracy: 0.9351\nEpoch 1995/2000\n10560/10560 [==============================] - 1s 70us/sample - loss: 0.1504 - accuracy: 0.9306\nEpoch 1996/2000\n10560/10560 [==============================] - 1s 75us/sample - loss: 0.1440 - accuracy: 0.9355\nEpoch 1997/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1509 - accuracy: 0.9310\nEpoch 1998/2000\n10560/10560 [==============================] - 1s 79us/sample - loss: 0.1501 - accuracy: 0.9300\nEpoch 1999/2000\n10560/10560 [==============================] - 1s 77us/sample - loss: 0.1799 - accuracy: 0.9170\nEpoch 2000/2000\n10560/10560 [==============================] - 1s 91us/sample - loss: 0.2151 - accuracy: 0.9084\n"
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Dense,Dropout, Conv1D, MaxPool1D, GlobalMaxPooling1D,Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#import keras_metrics\n",
    "print(\"Import Worked\")\n",
    "def create_model():\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D(250, 17, padding = 'valid', activation=\"relu\", input_shape=(len(df_scaled.columns)-1,1)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(30-1, activation='relu'))\n",
    "    model.add(Dense(30, activation='relu'))\n",
    "    model.add(Dense(2,activation=\"softmax\"))\n",
    "    \n",
    "    #COMPILE MODE\n",
    "    print(\"Output layer\")\n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam',metrics=[\"accuracy\"])\n",
    "    return model\n",
    " \n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "print(\"About to start estimator\")   \n",
    "\n",
    "keras_model = create_model()\n",
    "history = keras_model.fit(X_train,y_train,epochs=2000, batch_size=50,verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2640/2640 [==============================] - 0s 92us/sample - loss: 0.8673 - accuracy: 0.8458\n"
    }
   ],
   "source": [
    "test_loss,test_acc = keras_model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Import Worked\nAbout to start estimator\nOutput layer\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 195us/sample - loss: 1.2171 - accuracy: 0.6191\n2640/2640 [==============================] - 0s 97us/sample - loss: 0.6540 - accuracy: 0.6716\n[0.6539571686224503, 0.6715909]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 91us/sample - loss: 0.6396 - accuracy: 0.6739\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.6204 - accuracy: 0.6871\n[0.6203635674534422, 0.6871212]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 94us/sample - loss: 0.6258 - accuracy: 0.6826\n2640/2640 [==============================] - 0s 58us/sample - loss: 0.6134 - accuracy: 0.6943\n[0.6133595528024616, 0.6943182]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 110us/sample - loss: 0.6180 - accuracy: 0.6915\n2640/2640 [==============================] - 0s 54us/sample - loss: 0.6064 - accuracy: 0.6992\n[0.6063626448313395, 0.6992424]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.6133 - accuracy: 0.6953\n2640/2640 [==============================] - 0s 64us/sample - loss: 0.6026 - accuracy: 0.6955\n[0.6025846257354274, 0.69545454]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 91us/sample - loss: 0.6094 - accuracy: 0.6946\n2640/2640 [==============================] - 0s 54us/sample - loss: 0.6100 - accuracy: 0.6955\n[0.6100444974321307, 0.69545454]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.6072 - accuracy: 0.6989\n2640/2640 [==============================] - 0s 59us/sample - loss: 0.5997 - accuracy: 0.6939\n[0.5997451601606427, 0.6939394]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 100us/sample - loss: 0.6051 - accuracy: 0.6988\n2640/2640 [==============================] - 0s 51us/sample - loss: 0.5952 - accuracy: 0.7030\n[0.5951610453201063, 0.7030303]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 93us/sample - loss: 0.6040 - accuracy: 0.6999\n2640/2640 [==============================] - 0s 64us/sample - loss: 0.6083 - accuracy: 0.6932\n[0.6083181561845722, 0.6931818]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 95us/sample - loss: 0.6036 - accuracy: 0.6999\n2640/2640 [==============================] - 0s 60us/sample - loss: 0.6052 - accuracy: 0.6928\n[0.6051934310884187, 0.692803]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 113us/sample - loss: 0.6012 - accuracy: 0.6997\n2640/2640 [==============================] - 0s 99us/sample - loss: 0.5988 - accuracy: 0.6996\n[0.5987578384804003, 0.6996212]\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 145us/sample - loss: 0.6004 - accuracy: 0.7002\n2640/2640 [==============================] - 0s 94us/sample - loss: 0.5914 - accuracy: 0.6973\n[0.5913513133020112, 0.6973485]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 118us/sample - loss: 0.5998 - accuracy: 0.7007\n2640/2640 [==============================] - 0s 71us/sample - loss: 0.5969 - accuracy: 0.6996\n[0.5969379078258168, 0.6996212]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 142us/sample - loss: 0.5990 - accuracy: 0.6987\n2640/2640 [==============================] - 0s 81us/sample - loss: 0.5908 - accuracy: 0.6970\n[0.5907675212079828, 0.6969697]\nTrain on 10560 samples\n10560/10560 [==============================] - 2s 151us/sample - loss: 0.5985 - accuracy: 0.6964\n2640/2640 [==============================] - 0s 63us/sample - loss: 0.5909 - accuracy: 0.6973\n[0.5909165042819399, 0.6973485]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 98us/sample - loss: 0.5974 - accuracy: 0.6981\n2640/2640 [==============================] - 0s 71us/sample - loss: 0.5923 - accuracy: 0.6970\n[0.5923270337509386, 0.6969697]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 116us/sample - loss: 0.5972 - accuracy: 0.7002\n2640/2640 [==============================] - 0s 84us/sample - loss: 0.5887 - accuracy: 0.6996\n[0.5887136993986187, 0.6996212]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 103us/sample - loss: 0.5966 - accuracy: 0.6975\n2640/2640 [==============================] - 0s 69us/sample - loss: 0.5911 - accuracy: 0.6973\n[0.5911122524377072, 0.6973485]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 90us/sample - loss: 0.5956 - accuracy: 0.7014\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.5882 - accuracy: 0.7019\n[0.5882427213769971, 0.7018939]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.5954 - accuracy: 0.7018\n2640/2640 [==============================] - 0s 68us/sample - loss: 0.5875 - accuracy: 0.7011\n[0.5875015768137846, 0.70113635]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 100us/sample - loss: 0.5951 - accuracy: 0.6980\n2640/2640 [==============================] - 0s 70us/sample - loss: 0.5982 - accuracy: 0.6989\n[0.5981907241272204, 0.6988636]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 99us/sample - loss: 0.5954 - accuracy: 0.6988\n2640/2640 [==============================] - 0s 50us/sample - loss: 0.5868 - accuracy: 0.6996\n[0.5867516279220581, 0.6996212]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 97us/sample - loss: 0.5943 - accuracy: 0.6995\n2640/2640 [==============================] - 0s 61us/sample - loss: 0.5859 - accuracy: 0.7011\n[0.5859004855155945, 0.70113635]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 104us/sample - loss: 0.5939 - accuracy: 0.6999\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.5861 - accuracy: 0.7011\n[0.5860731986435976, 0.70113635]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 103us/sample - loss: 0.5935 - accuracy: 0.6987\n2640/2640 [==============================] - 0s 72us/sample - loss: 0.5903 - accuracy: 0.7030\n[0.5903275247776147, 0.7030303]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 90us/sample - loss: 0.5930 - accuracy: 0.6990\n2640/2640 [==============================] - 0s 60us/sample - loss: 0.5919 - accuracy: 0.6958\n[0.5918662558902393, 0.6958333]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 86us/sample - loss: 0.5931 - accuracy: 0.6980\n2640/2640 [==============================] - 0s 69us/sample - loss: 0.5862 - accuracy: 0.7015\n[0.5862171480149934, 0.70151514]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 108us/sample - loss: 0.5916 - accuracy: 0.7006\n2640/2640 [==============================] - 0s 52us/sample - loss: 0.5851 - accuracy: 0.7023\n[0.585126422029553, 0.7022727]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 93us/sample - loss: 0.5915 - accuracy: 0.7003\n2640/2640 [==============================] - 0s 64us/sample - loss: 0.5821 - accuracy: 0.7038\n[0.5821423050129052, 0.70378786]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 84us/sample - loss: 0.5905 - accuracy: 0.7006\n2640/2640 [==============================] - 0s 54us/sample - loss: 0.5860 - accuracy: 0.7049\n[0.5859810269240177, 0.7049242]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 85us/sample - loss: 0.5899 - accuracy: 0.7009\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.5821 - accuracy: 0.7011\n[0.5821188363161954, 0.70113635]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.5893 - accuracy: 0.7004\n2640/2640 [==============================] - 0s 63us/sample - loss: 0.5829 - accuracy: 0.7000\n[0.582855887485273, 0.7]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 85us/sample - loss: 0.5885 - accuracy: 0.7027\n2640/2640 [==============================] - 0s 67us/sample - loss: 0.5846 - accuracy: 0.7049\n[0.5845634400844574, 0.7049242]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 88us/sample - loss: 0.5879 - accuracy: 0.7031\n2640/2640 [==============================] - 0s 46us/sample - loss: 0.5830 - accuracy: 0.7042\n[0.5830343999645927, 0.70416665]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 91us/sample - loss: 0.5867 - accuracy: 0.7006\n2640/2640 [==============================] - 0s 50us/sample - loss: 0.5813 - accuracy: 0.7027\n[0.5812848562544043, 0.7026515]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 99us/sample - loss: 0.5873 - accuracy: 0.7017\n2640/2640 [==============================] - 0s 55us/sample - loss: 0.5807 - accuracy: 0.7004\n[0.5807266437646115, 0.7003788]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 93us/sample - loss: 0.5863 - accuracy: 0.7023\n2640/2640 [==============================] - 0s 76us/sample - loss: 0.5912 - accuracy: 0.6939\n[0.591237181966955, 0.6939394]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 104us/sample - loss: 0.5862 - accuracy: 0.7028\n2640/2640 [==============================] - 0s 59us/sample - loss: 0.5782 - accuracy: 0.7008\n[0.5781890867334424, 0.70075756]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 97us/sample - loss: 0.5857 - accuracy: 0.7019\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.5792 - accuracy: 0.6981\n[0.579226114171924, 0.69810605]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 102us/sample - loss: 0.5846 - accuracy: 0.7016\n2640/2640 [==============================] - 0s 68us/sample - loss: 0.5795 - accuracy: 0.7034\n[0.5795040641770218, 0.7034091]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.5837 - accuracy: 0.7018\n2640/2640 [==============================] - 0s 64us/sample - loss: 0.5802 - accuracy: 0.7011\n[0.5801597042517228, 0.70113635]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 109us/sample - loss: 0.5834 - accuracy: 0.7029\n2640/2640 [==============================] - 0s 68us/sample - loss: 0.5804 - accuracy: 0.7045\n[0.5804258500084732, 0.70454544]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.5826 - accuracy: 0.7027\n2640/2640 [==============================] - 0s 57us/sample - loss: 0.5882 - accuracy: 0.7019\n[0.5881943200573777, 0.7018939]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 99us/sample - loss: 0.5832 - accuracy: 0.7041\n2640/2640 [==============================] - 0s 55us/sample - loss: 0.5787 - accuracy: 0.7023\n[0.5786598689628369, 0.7022727]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 100us/sample - loss: 0.5826 - accuracy: 0.7030\n2640/2640 [==============================] - 0s 76us/sample - loss: 0.5787 - accuracy: 0.6992\n[0.5787211649345629, 0.6992424]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 88us/sample - loss: 0.5824 - accuracy: 0.7024\n2640/2640 [==============================] - 0s 50us/sample - loss: 0.5756 - accuracy: 0.7019\n[0.5756300207340356, 0.7018939]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 84us/sample - loss: 0.5817 - accuracy: 0.7047\n2640/2640 [==============================] - 0s 58us/sample - loss: 0.5765 - accuracy: 0.6977\n[0.5764756807775209, 0.69772726]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 85us/sample - loss: 0.5819 - accuracy: 0.7045\n2640/2640 [==============================] - 0s 65us/sample - loss: 0.5793 - accuracy: 0.7061\n[0.5792786885391582, 0.7060606]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 82us/sample - loss: 0.5815 - accuracy: 0.7016\n2640/2640 [==============================] - 0s 58us/sample - loss: 0.5762 - accuracy: 0.7068\n[0.576225392926823, 0.70681816]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 90us/sample - loss: 0.5803 - accuracy: 0.7027\n2640/2640 [==============================] - 0s 53us/sample - loss: 0.5757 - accuracy: 0.7042\n[0.5757005010590409, 0.70416665]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 82us/sample - loss: 0.5796 - accuracy: 0.7047\n2640/2640 [==============================] - 0s 55us/sample - loss: 0.5744 - accuracy: 0.7027\n[0.5744291820309378, 0.7026515]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 82us/sample - loss: 0.5792 - accuracy: 0.7049\n2640/2640 [==============================] - 0s 70us/sample - loss: 0.5754 - accuracy: 0.7023\n[0.5754067733432308, 0.7022727]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 84us/sample - loss: 0.5801 - accuracy: 0.7054\n2640/2640 [==============================] - 0s 63us/sample - loss: 0.5742 - accuracy: 0.7034\n[0.5742335684371717, 0.7034091]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 88us/sample - loss: 0.5791 - accuracy: 0.7037\n2640/2640 [==============================] - 0s 74us/sample - loss: 0.5775 - accuracy: 0.7019\n[0.5775010036699699, 0.7018939]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 92us/sample - loss: 0.5783 - accuracy: 0.7042\n2640/2640 [==============================] - 0s 50us/sample - loss: 0.5773 - accuracy: 0.7053\n[0.5773437575860457, 0.705303]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 97us/sample - loss: 0.5790 - accuracy: 0.7036\n2640/2640 [==============================] - 0s 51us/sample - loss: 0.5753 - accuracy: 0.7049\n[0.5752522802714145, 0.7049242]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 95us/sample - loss: 0.5785 - accuracy: 0.7045\n2640/2640 [==============================] - 0s 65us/sample - loss: 0.5736 - accuracy: 0.6985\n[0.5735660014730511, 0.69848484]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 99us/sample - loss: 0.5784 - accuracy: 0.7042\n2640/2640 [==============================] - 0s 70us/sample - loss: 0.5753 - accuracy: 0.7049\n[0.5753414219075983, 0.7049242]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 90us/sample - loss: 0.5773 - accuracy: 0.7051\n2640/2640 [==============================] - 0s 48us/sample - loss: 0.5734 - accuracy: 0.7023\n[0.573426081014402, 0.7022727]\nTrain on 10560 samples\n10560/10560 [==============================] - 1s 95us/sample - loss: 0.5783 - accuracy: 0.7039\n"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-119-4e8830fd1006>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    928\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    929\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 930\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    931\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m   def predict(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, model, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    488\u001b[0m         \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[0;32m    491\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m   def predict(self, model, x, batch_size=None, verbose=0, steps=None,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[1;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 426\u001b[1;33m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[1;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 706\u001b[1;33m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    707\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    708\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[0mdataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mflat_map\u001b[1;34m(self, map_func)\u001b[0m\n\u001b[0;32m   1613\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1614\u001b[0m     \"\"\"\n\u001b[1;32m-> 1615\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mFlatMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1616\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1617\u001b[0m   def interleave(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func)\u001b[0m\n\u001b[0;32m   3956\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_input_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3957\u001b[0m     self._map_func = StructuredFunctionWrapper(\n\u001b[1;32m-> 3958\u001b[1;33m         map_func, self._transformation_name(), dataset=input_dataset)\n\u001b[0m\u001b[0;32m   3959\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_map_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDatasetSpec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3960\u001b[0m       raise TypeError(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   3145\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3146\u001b[0m         \u001b[1;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3147\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3149\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2393\u001b[0m     \u001b[1;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[1;32m-> 2395\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   2396\u001b[0m     \u001b[1;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2397\u001b[0m     \u001b[1;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   2701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2703\u001b[1;33m       \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2705\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2593\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2595\u001b[0m         \u001b[1;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m    976\u001b[0m                                           converted_func)\n\u001b[0;32m    977\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 978\u001b[1;33m       \u001b[0mfunc_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m       \u001b[1;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mwrapper_fn\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3138\u001b[0m           attributes=defun_kwargs)\n\u001b[0;32m   3139\u001b[0m       \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=missing-docstring\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3140\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_wrapper_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3141\u001b[0m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_tensor_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output_structure\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3142\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_wrapper_helper\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m   3080\u001b[0m         \u001b[0mnested_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3081\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3082\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mautograph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mag_ctx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mnested_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3083\u001b[0m       \u001b[1;31m# If `func` returns a list of tensors, `nest.flatten()` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3084\u001b[0m       \u001b[1;31m# `ops.convert_to_tensor()` would conspire to attempt to stack\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    232\u001b[0m           optional_features=optional_features)\n\u001b[0;32m    233\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 234\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    235\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint:disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    236\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ag_error_metadata'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[1;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    458\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_whitelisted_for_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 459\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m   \u001b[1;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\autograph\\impl\\api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[1;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[0;32m    328\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    329\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 330\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    331\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    332\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mslice_batch_indices\u001b[1;34m(indices)\u001b[0m\n\u001b[0;32m    345\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_partial_batch_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    346\u001b[0m         index_remainder = dataset_ops.DatasetV2.from_tensors(array_ops.slice(\n\u001b[1;32m--> 347\u001b[1;33m             indices, [num_in_full_batch], [self._partial_batch_size]))\n\u001b[0m\u001b[0;32m    348\u001b[0m         \u001b[0mflat_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mflat_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex_remainder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    349\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\array_ops.py\u001b[0m in \u001b[0;36mslice\u001b[1;34m(input_, begin, size, name)\u001b[0m\n\u001b[0;32m    949\u001b[0m     \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msame\u001b[0m \u001b[0mtype\u001b[0m \u001b[1;32mas\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0minput_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    950\u001b[0m   \"\"\"\n\u001b[1;32m--> 951\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mgen_array_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbegin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    952\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    953\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\ops\\gen_array_ops.py\u001b[0m in \u001b[0;36m_slice\u001b[1;34m(input, begin, size, name)\u001b[0m\n\u001b[0;32m   8449\u001b[0m   \u001b[1;31m# Add nodes to the TensorFlow graph.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8450\u001b[0m   _, _, _op, _outputs = _op_def_library._apply_op_helper(\n\u001b[1;32m-> 8451\u001b[1;33m         \"Slice\", input=input, begin=begin, size=size, name=name)\n\u001b[0m\u001b[0;32m   8452\u001b[0m   \u001b[0m_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   8453\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[1;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[0;32m    466\u001b[0m               \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m               \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_arg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_ref\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m               preferred_dtype=default_dtype)\n\u001b[0m\u001b[0;32m    469\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1313\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1314\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_tensor_conversion_function\u001b[1;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[0;32m    315\u001b[0m                                          as_ref=False):\n\u001b[0;32m    316\u001b[0m   \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    256\u001b[0m   \"\"\"\n\u001b[0;32m    257\u001b[0m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[1;32m--> 258\u001b[1;33m                         allow_broadcast=True)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    300\u001b[0m       attrs={\"value\": tensor_value,\n\u001b[0;32m    301\u001b[0m              \"dtype\": dtype_value},\n\u001b[1;32m--> 302\u001b[1;33m       name=name).outputs[0]\n\u001b[0m\u001b[0;32m    303\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mconst_tensor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m    593\u001b[0m     return super(FuncGraph, self)._create_op_internal(  # pylint: disable=protected-access\n\u001b[0;32m    594\u001b[0m         \u001b[0mop_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_def\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 595\u001b[1;33m         compute_device)\n\u001b[0m\u001b[0;32m    596\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    597\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mcapture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_op_internal\u001b[1;34m(self, op_type, inputs, dtypes, input_types, name, attrs, op_def, compute_device)\u001b[0m\n\u001b[0;32m   3320\u001b[0m           \u001b[0minput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3321\u001b[0m           \u001b[0moriginal_op\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_default_original_op\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3322\u001b[1;33m           op_def=op_def)\n\u001b[0m\u001b[0;32m   3323\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_op_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mret\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcompute_device\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3324\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mret\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, node_def, g, inputs, output_types, control_inputs, input_types, original_op, op_def)\u001b[0m\n\u001b[0;32m   1784\u001b[0m           op_def, inputs, node_def.attr)\n\u001b[0;32m   1785\u001b[0m       self._c_op = _create_c_op(self._graph, node_def, grouped_inputs,\n\u001b[1;32m-> 1786\u001b[1;33m                                 control_input_ops)\n\u001b[0m\u001b[0;32m   1787\u001b[0m       \u001b[0mname\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1788\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_create_c_op\u001b[1;34m(graph, node_def, inputs, control_inputs)\u001b[0m\n\u001b[0;32m   1614\u001b[0m     \u001b[1;31m# TODO(skyewm): this creates and deletes a new TF_Status for every attr.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1615\u001b[0m     \u001b[1;31m# It might be worth creating a convenient way to re-use the same status.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1616\u001b[1;33m     \u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_SetAttrValueProto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop_desc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mserialized\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1617\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1618\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.layers import Dense,Dropout, Conv1D, MaxPool1D, GlobalMaxPooling1D,Embedding\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras import regularizers\n",
    "from sklearn.model_selection import cross_val_score\n",
    "#import keras_metrics\n",
    "print(\"Import Worked\")\n",
    "def create_model():\n",
    "    model=Sequential()\n",
    "    model.add(Conv1D(250, 17, padding = 'valid', activation=\"relu\", input_shape=(len(df_scaled.columns)-1,1)))\n",
    "    model.add(GlobalMaxPooling1D())\n",
    "    # model.add(Conv1D(250, 17, padding = 'valid', activation=\"relu\", input_shape=(250,17)))\n",
    "    # model.add(GlobalMaxPooling1D())\n",
    "    model.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(100, activation='relu',kernel_regularizer=regularizers.l2(0.01)))\n",
    "    model.add(Dense(1,activation=\"sigmoid\"))\n",
    "    \n",
    "    #COMPILE MODE\n",
    "    print(\"Output layer\")\n",
    "    model.compile(loss='binary_crossentropy', optimizer='nadam',metrics=[\"accuracy\"])\n",
    "    return model\n",
    " \n",
    "from tensorflow.python.keras.wrappers.scikit_learn import KerasClassifier\n",
    "print(\"About to start estimator\")   \n",
    "\n",
    "model = create_model()\n",
    "for i in range(2000):\n",
    "    model.fit(X_train,y_train,epochs=1, batch_size=50,verbose=1)\n",
    "    print(model.evaluate(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "olute_error: 0.1635 - acc: 0.0016\n",
      "Epoch 1855/2000\n",
      "3685/3685 [==============================] - 1s 309us/sample - loss: 0.0458 - mean_absolute_error: 0.1384 - acc: 0.0016\n",
      "Epoch 1856/2000\n",
      "3685/3685 [==============================] - 1s 295us/sample - loss: 0.0855 - mean_absolute_error: 0.2112 - acc: 0.0016\n",
      "Epoch 1857/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0642 - mean_absolute_error: 0.1691 - acc: 0.0016\n",
      "Epoch 1858/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0534 - mean_absolute_error: 0.1553 - acc: 0.0016\n",
      "Epoch 1859/2000\n",
      "3685/3685 [==============================] - 1s 303us/sample - loss: 0.0512 - mean_absolute_error: 0.1535 - acc: 0.0016\n",
      "Epoch 1860/2000\n",
      "3685/3685 [==============================] - 1s 293us/sample - loss: 0.0510 - mean_absolute_error: 0.1529 - acc: 0.0016\n",
      "Epoch 1861/2000\n",
      "3685/3685 [==============================] - 1s 287us/sample - loss: 0.0532 - mean_absolute_error: 0.1570 - acc: 0.0016\n",
      "Epoch 1862/2000\n",
      "3685/3685 [==============================] - 1s 276us/sample - loss: 0.0607 - mean_absolute_error: 0.1662 - acc: 0.0016\n",
      "Epoch 1863/2000\n",
      "3685/3685 [==============================] - 1s 293us/sample - loss: 0.0730 - mean_absolute_error: 0.1804 - acc: 0.0016\n",
      "Epoch 1864/2000\n",
      "3685/3685 [==============================] - 1s 291us/sample - loss: 0.0569 - mean_absolute_error: 0.1637 - acc: 0.0016\n",
      "Epoch 1865/2000\n",
      "3685/3685 [==============================] - 1s 292us/sample - loss: 0.0493 - mean_absolute_error: 0.1483 - acc: 0.0016\n",
      "Epoch 1866/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0465 - mean_absolute_error: 0.1410 - acc: 0.0016\n",
      "Epoch 1867/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0748 - mean_absolute_error: 0.1917 - acc: 0.0016\n",
      "Epoch 1868/2000\n",
      "3685/3685 [==============================] - 1s 292us/sample - loss: 0.0620 - mean_absolute_error: 0.1691 - acc: 0.0016\n",
      "Epoch 1869/2000\n",
      "3685/3685 [==============================] - 1s 288us/sample - loss: 0.0512 - mean_absolute_error: 0.1505 - acc: 0.0016\n",
      "Epoch 1870/2000\n",
      "3685/3685 [==============================] - 1s 297us/sample - loss: 0.0492 - mean_absolute_error: 0.1464 - acc: 0.0016\n",
      "Epoch 1871/2000\n",
      "3685/3685 [==============================] - 1s 285us/sample - loss: 0.0552 - mean_absolute_error: 0.1569 - acc: 0.0016\n",
      "Epoch 1872/2000\n",
      "3685/3685 [==============================] - 1s 297us/sample - loss: 0.0551 - mean_absolute_error: 0.1613 - acc: 0.0016\n",
      "Epoch 1873/2000\n",
      "3685/3685 [==============================] - 1s 329us/sample - loss: 0.0583 - mean_absolute_error: 0.1699 - acc: 0.0016\n",
      "Epoch 1874/2000\n",
      "3685/3685 [==============================] - 1s 305us/sample - loss: 0.0823 - mean_absolute_error: 0.2107 - acc: 0.0016\n",
      "Epoch 1875/2000\n",
      "3685/3685 [==============================] - 1s 290us/sample - loss: 0.0527 - mean_absolute_error: 0.1565 - acc: 0.0016\n",
      "Epoch 1876/2000\n",
      "3685/3685 [==============================] - 1s 296us/sample - loss: 0.0510 - mean_absolute_error: 0.1523 - acc: 0.0016\n",
      "Epoch 1877/2000\n",
      "3685/3685 [==============================] - 1s 290us/sample - loss: 0.0463 - mean_absolute_error: 0.1411 - acc: 0.0016\n",
      "Epoch 1878/2000\n",
      "3685/3685 [==============================] - 1s 287us/sample - loss: 0.0671 - mean_absolute_error: 0.1846 - acc: 0.0016\n",
      "Epoch 1879/2000\n",
      "3685/3685 [==============================] - 1s 298us/sample - loss: 0.0670 - mean_absolute_error: 0.1776 - acc: 0.0016\n",
      "Epoch 1880/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0553 - mean_absolute_error: 0.1586 - acc: 0.0016\n",
      "Epoch 1881/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0602 - mean_absolute_error: 0.1654 - acc: 0.0016\n",
      "Epoch 1882/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0563 - mean_absolute_error: 0.1649 - acc: 0.0016\n",
      "Epoch 1883/2000\n",
      "3685/3685 [==============================] - 1s 304us/sample - loss: 0.0587 - mean_absolute_error: 0.1711 - acc: 0.0016\n",
      "Epoch 1884/2000\n",
      "3685/3685 [==============================] - 1s 295us/sample - loss: 0.0769 - mean_absolute_error: 0.1962 - acc: 0.0016\n",
      "Epoch 1885/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0526 - mean_absolute_error: 0.1526 - acc: 0.0016\n",
      "Epoch 1886/2000\n",
      "3685/3685 [==============================] - 1s 294us/sample - loss: 0.0581 - mean_absolute_error: 0.1657 - acc: 0.0016\n",
      "Epoch 1887/2000\n",
      "3685/3685 [==============================] - 1s 298us/sample - loss: 0.0593 - mean_absolute_error: 0.1627 - acc: 0.0016\n",
      "Epoch 1888/2000\n",
      "3685/3685 [==============================] - 1s 303us/sample - loss: 0.0519 - mean_absolute_error: 0.1567 - acc: 0.0016\n",
      "Epoch 1889/2000\n",
      "3685/3685 [==============================] - 1s 282us/sample - loss: 0.0522 - mean_absolute_error: 0.1483 - acc: 0.0016\n",
      "Epoch 1890/2000\n",
      "3685/3685 [==============================] - 1s 284us/sample - loss: 0.0662 - mean_absolute_error: 0.1788 - acc: 0.0016\n",
      "Epoch 1891/2000\n",
      "3685/3685 [==============================] - 1s 293us/sample - loss: 0.0644 - mean_absolute_error: 0.1741 - acc: 0.0016\n",
      "Epoch 1892/2000\n",
      "3685/3685 [==============================] - 1s 286us/sample - loss: 0.0652 - mean_absolute_error: 0.1793 - acc: 0.0016\n",
      "Epoch 1893/2000\n",
      "3685/3685 [==============================] - 1s 302us/sample - loss: 0.0584 - mean_absolute_error: 0.1661 - acc: 0.0016\n",
      "Epoch 1894/2000\n",
      "3685/3685 [==============================] - 1s 313us/sample - loss: 0.0435 - mean_absolute_error: 0.1369 - acc: 0.0016\n",
      "Epoch 1895/2000\n",
      "3685/3685 [==============================] - 1s 282us/sample - loss: 0.0525 - mean_absolute_error: 0.1568 - acc: 0.0016\n",
      "Epoch 1896/2000\n",
      "3685/3685 [==============================] - 1s 297us/sample - loss: 0.0425 - mean_absolute_error: 0.1364 - acc: 0.0016\n",
      "Epoch 1897/2000\n",
      "3685/3685 [==============================] - 1s 298us/sample - loss: 0.0628 - mean_absolute_error: 0.1683 - acc: 0.0016\n",
      "Epoch 1898/2000\n",
      "3685/3685 [==============================] - 1s 286us/sample - loss: 0.0545 - mean_absolute_error: 0.1540 - acc: 0.0016\n",
      "Epoch 1899/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0532 - mean_absolute_error: 0.1592 - acc: 0.0016\n",
      "Epoch 1900/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0533 - mean_absolute_error: 0.1575 - acc: 0.0016\n",
      "Epoch 1901/2000\n",
      "3685/3685 [==============================] - 1s 301us/sample - loss: 0.0877 - mean_absolute_error: 0.2052 - acc: 0.0016\n",
      "Epoch 1902/2000\n",
      "3685/3685 [==============================] - 1s 306us/sample - loss: 0.0490 - mean_absolute_error: 0.1473 - acc: 0.0016\n",
      "Epoch 1903/2000\n",
      "3685/3685 [==============================] - 1s 283us/sample - loss: 0.0593 - mean_absolute_error: 0.1639 - acc: 0.0016\n",
      "Epoch 1904/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0768 - mean_absolute_error: 0.1891 - acc: 0.0016\n",
      "Epoch 1905/2000\n",
      "3685/3685 [==============================] - 1s 296us/sample - loss: 0.0609 - mean_absolute_error: 0.1660 - acc: 0.0016\n",
      "Epoch 1906/2000\n",
      "3685/3685 [==============================] - 1s 292us/sample - loss: 0.0509 - mean_absolute_error: 0.1482 - acc: 0.0016\n",
      "Epoch 1907/2000\n",
      "3685/3685 [==============================] - 1s 285us/sample - loss: 0.0540 - mean_absolute_error: 0.1534 - acc: 0.0016\n",
      "Epoch 1908/2000\n",
      "3685/3685 [==============================] - 1s 286us/sample - loss: 0.0445 - mean_absolute_error: 0.1372 - acc: 0.0016\n",
      "Epoch 1909/2000\n",
      "3685/3685 [==============================] - 1s 276us/sample - loss: 0.0591 - mean_absolute_error: 0.1742 - acc: 0.0016\n",
      "Epoch 1910/2000\n",
      "3685/3685 [==============================] - 1s 302us/sample - loss: 0.0617 - mean_absolute_error: 0.1704 - acc: 0.0016\n",
      "Epoch 1911/2000\n",
      "3685/3685 [==============================] - 1s 291us/sample - loss: 0.0693 - mean_absolute_error: 0.1845 - acc: 0.0016\n",
      "Epoch 1912/2000\n",
      "3685/3685 [==============================] - 1s 287us/sample - loss: 0.0656 - mean_absolute_error: 0.1769 - acc: 0.0016\n",
      "Epoch 1913/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0608 - mean_absolute_error: 0.1695 - acc: 0.0016\n",
      "Epoch 1914/2000\n",
      "3685/3685 [==============================] - 1s 291us/sample - loss: 0.0610 - mean_absolute_error: 0.1694 - acc: 0.0016\n",
      "Epoch 1915/2000\n",
      "3685/3685 [==============================] - 1s 282us/sample - loss: 0.0564 - mean_absolute_error: 0.1568 - acc: 0.0016\n",
      "Epoch 1916/2000\n",
      "3685/3685 [==============================] - 1s 309us/sample - loss: 0.0794 - mean_absolute_error: 0.1972 - acc: 0.0016\n",
      "Epoch 1917/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0551 - mean_absolute_error: 0.1557 - acc: 0.0016\n",
      "Epoch 1918/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0623 - mean_absolute_error: 0.1705 - acc: 0.0016\n",
      "Epoch 1919/2000\n",
      "3685/3685 [==============================] - 1s 320us/sample - loss: 0.0645 - mean_absolute_error: 0.1725 - acc: 0.0016\n",
      "Epoch 1920/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0592 - mean_absolute_error: 0.1659 - acc: 0.0016\n",
      "Epoch 1921/2000\n",
      "3685/3685 [==============================] - 1s 289us/sample - loss: 0.0511 - mean_absolute_error: 0.1511 - acc: 0.0016\n",
      "Epoch 1922/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0431 - mean_absolute_error: 0.1365 - acc: 0.0016\n",
      "Epoch 1923/2000\n",
      "3685/3685 [==============================] - 1s 284us/sample - loss: 0.0599 - mean_absolute_error: 0.1728 - acc: 0.0016\n",
      "Epoch 1924/2000\n",
      "3685/3685 [==============================] - 1s 296us/sample - loss: 0.0556 - mean_absolute_error: 0.1610 - acc: 0.0016\n",
      "Epoch 1925/2000\n",
      "3685/3685 [==============================] - 1s 296us/sample - loss: 0.0725 - mean_absolute_error: 0.1831 - acc: 0.0016\n",
      "Epoch 1926/2000\n",
      "3685/3685 [==============================] - 1s 286us/sample - loss: 0.0593 - mean_absolute_error: 0.1622 - acc: 0.0016\n",
      "Epoch 1927/2000\n",
      "3685/3685 [==============================] - 1s 282us/sample - loss: 0.0591 - mean_absolute_error: 0.1667 - acc: 0.0016\n",
      "Epoch 1928/2000\n",
      "3685/3685 [==============================] - 1s 302us/sample - loss: 0.0541 - mean_absolute_error: 0.1561 - acc: 0.0016\n",
      "Epoch 1929/2000\n",
      "3685/3685 [==============================] - 1s 290us/sample - loss: 0.0548 - mean_absolute_error: 0.1580 - acc: 0.0016\n",
      "Epoch 1930/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0569 - mean_absolute_error: 0.1624 - acc: 0.0016\n",
      "Epoch 1931/2000\n",
      "3685/3685 [==============================] - 1s 276us/sample - loss: 0.0531 - mean_absolute_error: 0.1565 - acc: 0.0016\n",
      "Epoch 1932/2000\n",
      "3685/3685 [==============================] - 1s 301us/sample - loss: 0.0560 - mean_absolute_error: 0.1618 - acc: 0.0016\n",
      "Epoch 1933/2000\n",
      "3685/3685 [==============================] - 1s 318us/sample - loss: 0.0774 - mean_absolute_error: 0.1924 - acc: 0.0016\n",
      "Epoch 1934/2000\n",
      "3685/3685 [==============================] - 1s 317us/sample - loss: 0.0535 - mean_absolute_error: 0.1557 - acc: 0.0016\n",
      "Epoch 1935/2000\n",
      "3685/3685 [==============================] - 1s 309us/sample - loss: 0.0432 - mean_absolute_error: 0.1360 - acc: 0.0016\n",
      "Epoch 1936/2000\n",
      "3685/3685 [==============================] - 1s 290us/sample - loss: 0.0514 - mean_absolute_error: 0.1576 - acc: 0.0016\n",
      "Epoch 1937/2000\n",
      "3685/3685 [==============================] - 1s 292us/sample - loss: 0.0470 - mean_absolute_error: 0.1448 - acc: 0.0016\n",
      "Epoch 1938/2000\n",
      "3650/3685 [============================>.] - ETA: 0s - loss: 0.0753 - mean_absolute_error: 0.1920 - acc: 0.0013685/3685 [==============================] - 1s 285us/sample - loss: 0.0755 - mean_absolute_error: 0.1921 - acc: 0.0016\n",
      "Epoch 1939/2000\n",
      "3685/3685 [==============================] - 1s 305us/sample - loss: 0.0700 - mean_absolute_error: 0.1840 - acc: 0.0016\n",
      "Epoch 1940/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0758 - mean_absolute_error: 0.1720 - acc: 0.0016\n",
      "Epoch 1941/2000\n",
      "3685/3685 [==============================] - 1s 279us/sample - loss: 0.0692 - mean_absolute_error: 0.1731 - acc: 0.0016\n",
      "Epoch 1942/2000\n",
      "3685/3685 [==============================] - 1s 304us/sample - loss: 0.0629 - mean_absolute_error: 0.1698 - acc: 0.0016\n",
      "Epoch 1943/2000\n",
      "3685/3685 [==============================] - 1s 306us/sample - loss: 0.0511 - mean_absolute_error: 0.1494 - acc: 0.0016\n",
      "Epoch 1944/2000\n",
      "3685/3685 [==============================] - 1s 288us/sample - loss: 0.0830 - mean_absolute_error: 0.1961 - acc: 0.0016\n",
      "Epoch 1945/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0653 - mean_absolute_error: 0.1733 - acc: 0.0016\n",
      "Epoch 1946/2000\n",
      "3685/3685 [==============================] - 1s 283us/sample - loss: 0.0462 - mean_absolute_error: 0.1414 - acc: 0.0016\n",
      "Epoch 1947/2000\n",
      "3685/3685 [==============================] - 1s 289us/sample - loss: 0.0511 - mean_absolute_error: 0.1482 - acc: 0.0016\n",
      "Epoch 1948/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0595 - mean_absolute_error: 0.1666 - acc: 0.0016\n",
      "Epoch 1949/2000\n",
      "3685/3685 [==============================] - 1s 283us/sample - loss: 0.0863 - mean_absolute_error: 0.2058 - acc: 0.0016\n",
      "Epoch 1950/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0521 - mean_absolute_error: 0.1542 - acc: 0.0016\n",
      "Epoch 1951/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0613 - mean_absolute_error: 0.1710 - acc: 0.0016\n",
      "Epoch 1952/2000\n",
      "3685/3685 [==============================] - 1s 285us/sample - loss: 0.0487 - mean_absolute_error: 0.1465 - acc: 0.0016\n",
      "Epoch 1953/2000\n",
      "3685/3685 [==============================] - 1s 300us/sample - loss: 0.0540 - mean_absolute_error: 0.1537 - acc: 0.0016\n",
      "Epoch 1954/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0502 - mean_absolute_error: 0.1517 - acc: 0.0016\n",
      "Epoch 1955/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0494 - mean_absolute_error: 0.1483 - acc: 0.0016\n",
      "Epoch 1956/2000\n",
      "3685/3685 [==============================] - 1s 302us/sample - loss: 0.0561 - mean_absolute_error: 0.1673 - acc: 0.0016\n",
      "Epoch 1957/2000\n",
      "3685/3685 [==============================] - 1s 292us/sample - loss: 0.0503 - mean_absolute_error: 0.1532 - acc: 0.0016\n",
      "Epoch 1958/2000\n",
      "3685/3685 [==============================] - 1s 287us/sample - loss: 0.0614 - mean_absolute_error: 0.1727 - acc: 0.0016\n",
      "Epoch 1959/2000\n",
      "3685/3685 [==============================] - 1s 278us/sample - loss: 0.0523 - mean_absolute_error: 0.1548 - acc: 0.0016\n",
      "Epoch 1960/2000\n",
      "3685/3685 [==============================] - 1s 296us/sample - loss: 0.0511 - mean_absolute_error: 0.1465 - acc: 0.0016\n",
      "Epoch 1961/2000\n",
      "3685/3685 [==============================] - 1s 290us/sample - loss: 0.0647 - mean_absolute_error: 0.1772 - acc: 0.0016\n",
      "Epoch 1962/2000\n",
      "3685/3685 [==============================] - 1s 298us/sample - loss: 0.0490 - mean_absolute_error: 0.1483 - acc: 0.0016\n",
      "Epoch 1963/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0462 - mean_absolute_error: 0.1400 - acc: 0.0016\n",
      "Epoch 1964/2000\n",
      "3685/3685 [==============================] - 1s 279us/sample - loss: 0.0588 - mean_absolute_error: 0.1737 - acc: 0.0016\n",
      "Epoch 1965/2000\n",
      "3685/3685 [==============================] - 1s 295us/sample - loss: 0.0751 - mean_absolute_error: 0.1919 - acc: 0.0016\n",
      "Epoch 1966/2000\n",
      "3685/3685 [==============================] - 1s 283us/sample - loss: 0.0532 - mean_absolute_error: 0.1556 - acc: 0.0016\n",
      "Epoch 1967/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0563 - mean_absolute_error: 0.1642 - acc: 0.0016\n",
      "Epoch 1968/2000\n",
      "3685/3685 [==============================] - 1s 279us/sample - loss: 0.0593 - mean_absolute_error: 0.1698 - acc: 0.0016\n",
      "Epoch 1969/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0480 - mean_absolute_error: 0.1446 - acc: 0.0016\n",
      "Epoch 1970/2000\n",
      "3685/3685 [==============================] - 1s 317us/sample - loss: 0.0495 - mean_absolute_error: 0.1473 - acc: 0.0016\n",
      "Epoch 1971/2000\n",
      "3685/3685 [==============================] - 1s 298us/sample - loss: 0.0486 - mean_absolute_error: 0.1531 - acc: 0.0016\n",
      "Epoch 1972/2000\n",
      "3685/3685 [==============================] - 1s 283us/sample - loss: 0.0649 - mean_absolute_error: 0.1832 - acc: 0.0016\n",
      "Epoch 1973/2000\n",
      "3685/3685 [==============================] - 1s 282us/sample - loss: 0.0685 - mean_absolute_error: 0.1853 - acc: 0.0016\n",
      "Epoch 1974/2000\n",
      "3685/3685 [==============================] - 1s 287us/sample - loss: 0.0668 - mean_absolute_error: 0.1711 - acc: 0.0016\n",
      "Epoch 1975/2000\n",
      "3685/3685 [==============================] - 1s 297us/sample - loss: 0.0813 - mean_absolute_error: 0.1911 - acc: 0.0016\n",
      "Epoch 1976/2000\n",
      "3685/3685 [==============================] - 1s 299us/sample - loss: 0.0701 - mean_absolute_error: 0.1801 - acc: 0.0016\n",
      "Epoch 1977/2000\n",
      "3685/3685 [==============================] - 1s 286us/sample - loss: 0.0491 - mean_absolute_error: 0.1447 - acc: 0.0016\n",
      "Epoch 1978/2000\n",
      "3685/3685 [==============================] - 1s 276us/sample - loss: 0.0500 - mean_absolute_error: 0.1520 - acc: 0.0016\n",
      "Epoch 1979/2000\n",
      "3685/3685 [==============================] - 1s 298us/sample - loss: 0.0468 - mean_absolute_error: 0.1459 - acc: 0.0016\n",
      "Epoch 1980/2000\n",
      "3685/3685 [==============================] - 1s 287us/sample - loss: 0.0634 - mean_absolute_error: 0.1732 - acc: 0.0016\n",
      "Epoch 1981/2000\n",
      "3685/3685 [==============================] - 1s 306us/sample - loss: 0.0622 - mean_absolute_error: 0.1696 - acc: 0.0016\n",
      "Epoch 1982/2000\n",
      "3685/3685 [==============================] - 1s 282us/sample - loss: 0.0510 - mean_absolute_error: 0.1530 - acc: 0.0016\n",
      "Epoch 1983/2000\n",
      "3685/3685 [==============================] - 1s 279us/sample - loss: 0.0586 - mean_absolute_error: 0.1663 - acc: 0.0016\n",
      "Epoch 1984/2000\n",
      "3685/3685 [==============================] - 1s 313us/sample - loss: 0.0767 - mean_absolute_error: 0.1948 - acc: 0.0016\n",
      "Epoch 1985/2000\n",
      "3685/3685 [==============================] - 1s 297us/sample - loss: 0.0529 - mean_absolute_error: 0.1540 - acc: 0.0016\n",
      "Epoch 1986/2000\n",
      "3685/3685 [==============================] - 1s 283us/sample - loss: 0.0444 - mean_absolute_error: 0.1378 - acc: 0.0016\n",
      "Epoch 1987/2000\n",
      "3685/3685 [==============================] - 1s 295us/sample - loss: 0.0580 - mean_absolute_error: 0.1659 - acc: 0.0016\n",
      "Epoch 1988/2000\n",
      "3685/3685 [==============================] - 1s 294us/sample - loss: 0.0601 - mean_absolute_error: 0.1691 - acc: 0.0016\n",
      "Epoch 1989/2000\n",
      "3685/3685 [==============================] - 1s 288us/sample - loss: 0.0514 - mean_absolute_error: 0.1502 - acc: 0.0016\n",
      "Epoch 1990/2000\n",
      "3685/3685 [==============================] - 1s 296us/sample - loss: 0.0485 - mean_absolute_error: 0.1498 - acc: 0.0016\n",
      "Epoch 1991/2000\n",
      "3685/3685 [==============================] - 1s 281us/sample - loss: 0.0445 - mean_absolute_error: 0.1425 - acc: 0.0016\n",
      "Epoch 1992/2000\n",
      "3685/3685 [==============================] - 1s 286us/sample - loss: 0.0634 - mean_absolute_error: 0.1672 - acc: 0.0016\n",
      "Epoch 1993/2000\n",
      "3685/3685 [==============================] - 1s 321us/sample - loss: 0.0667 - mean_absolute_error: 0.1758 - acc: 0.0016\n",
      "Epoch 1994/2000\n",
      "3685/3685 [==============================] - 1s 326us/sample - loss: 0.0698 - mean_absolute_error: 0.1794 - acc: 0.0016\n",
      "Epoch 1995/2000\n",
      "3685/3685 [==============================] - 1s 325us/sample - loss: 0.0662 - mean_absolute_error: 0.1771 - acc: 0.0016\n",
      "Epoch 1996/2000\n",
      "3685/3685 [==============================] - 1s 302us/sample - loss: 0.0666 - mean_absolute_error: 0.1748 - acc: 0.0016\n",
      "Epoch 1997/2000\n",
      "3685/3685 [==============================] - 1s 295us/sample - loss: 0.0536 - mean_absolute_error: 0.1578 - acc: 0.0016\n",
      "Epoch 1998/2000\n",
      "3685/3685 [==============================] - 1s 289us/sample - loss: 0.0591 - mean_absolute_error: 0.1669 - acc: 0.0016\n",
      "Epoch 1999/2000\n",
      "3685/3685 [==============================] - 1s 306us/sample - loss: 0.0493 - mean_absolute_error: 0.1492 - acc: 0.0016\n",
      "Epoch 2000/2000\n",
      "3685/3685 [==============================] - 1s 280us/sample - loss: 0.0523 - mean_absolute_error: 0.1513 - acc: 0.0016\n"
     ]
    }
   ],
   "source": [
    "history = keras_model.fit(X_train,y_train,epochs=2000, batch_size=50,verbose=1)"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbasecondaccb0c1180edc481782339d9fb6e461e1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}